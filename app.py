import streamlit as st
import pandas as pd
import time
import matplotlib.pyplot as plt
from openpyxl.utils.dataframe import dataframe_to_rows
import io
from rapidfuzz import fuzz
import os
from openpyxl import load_workbook
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from io import StringIO, BytesIO
import sys
import contextlib
from langchain_openai import ChatOpenAI  # Updated import
import pdfkit
from jinja2 import Template
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Optional
import torch
from transformers import (
    pipeline,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    AutoModelForCausalLM  # 4 Qwen
)

from threading import Event
import threading
from queue import Queue

class ProcessControl:
    def __init__(self):
        self.pause_event = Event()
        self.stop_event = Event()
        self.pause_event.set()  # Start in non-paused state
        
    def pause(self):
        self.pause_event.clear()
        
    def resume(self):
        self.pause_event.set()
        
    def stop(self):
        self.stop_event.set()
        self.pause_event.set()  # Ensure not stuck in pause
        
    def reset(self):
        self.stop_event.clear()
        self.pause_event.set()
        
    def is_paused(self):
        return not self.pause_event.is_set()
        
    def is_stopped(self):
        return self.stop_event.is_set()
        
    def wait_if_paused(self):
        self.pause_event.wait()


class FallbackLLMSystem:
    def __init__(self):
        """Initialize fallback models for event detection and reasoning"""
        try:
            # Initialize MT5 model (multilingual T5)
            self.model_name = "google/mt5-small"
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            
            # Set device
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = self.model.to(self.device)
            
            st.success(f"–ó–∞–ø—É—Å—Ç–∏–ª MT5-–º–æ–¥–µ–ª—å –Ω–∞ {self.device}")
            
        except Exception as e:
            st.error(f"Error initializing MT5: {str(e)}")
            raise

    def detect_events(self, text, entity):
        """Detect events using MT5"""
        # Initialize default return values
        event_type = "–ù–µ—Ç"
        summary = ""
        
        try:
            prompt = f"""<s>Analyze news about company {entity}:

            {text}

            Classify event type as one of:
            - –û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å (financial reports)
            - –†–¶–ë (securities market events)
            - –°—É–¥ (legal actions)
            - –ù–µ—Ç (no significant events)

            Format response as:
            –¢–∏–ø: [type]
            –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: [summary]</s>"""
                        
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            outputs = self.model.generate(
                **inputs,
                max_length=200,
                num_return_sequences=1,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id
            )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Parse response
            if "–¢–∏–ø:" in response and "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:" in response:
                parts = response.split("–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:")
                type_part = parts[0]
                if "–¢–∏–ø:" in type_part:
                    event_type = type_part.split("–¢–∏–ø:")[1].strip()
                    # Validate event type
                    valid_types = ["–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "–†–¶–ë", "–°—É–¥", "–ù–µ—Ç"]
                    if event_type not in valid_types:
                        event_type = "–ù–µ—Ç"
                
                if len(parts) > 1:
                    summary = parts[1].strip()
            
            return event_type, summary
            
        except Exception as e:
            st.warning(f"Event detection error: {str(e)}")
            return "–ù–µ—Ç", "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"

def ensure_groq_llm():
    """Initialize Groq LLM for impact estimation"""
    try:
        if 'groq_key' not in st.secrets:
            st.error("Groq API key not found in secrets. Please add it with the key 'groq_key'.")
            return None
            
        return ChatOpenAI(
            base_url="https://api.groq.com/openai/v1",
            model="llama-3.1-70b-versatile",
            openai_api_key=st.secrets['groq_key'],
            temperature=0.0
        )
    except Exception as e:
        st.error(f"Error initializing Groq LLM: {str(e)}")
        return None

def estimate_impact(llm, news_text, entity):
    """
    Estimate impact using Groq LLM regardless of the main model choice.
    Falls back to the provided LLM if Groq initialization fails.
    """
    # Initialize default return values
    impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
    reasoning = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    
    try:
        # Always try to use Groq first
        groq_llm = ensure_groq_llm()
        working_llm = groq_llm if groq_llm is not None else llm
        
        template = """
        You are a financial analyst. Analyze this news piece about {entity} and assess its potential impact.
        
        News: {news}
        
        Classify the impact into one of these categories:
        1. "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Significant loss risk)
        2. "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Moderate loss risk)
        3. "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Minor loss risk)
        4. "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏" (Potential profit)
        5. "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç" (Uncertain effect)
        
        Provide a brief, fact-based reasoning for your assessment.
        
        Format your response exactly as:
        Impact: [category]
        Reasoning: [explanation in 2-3 sentences]
        """
        
        prompt = PromptTemplate(template=template, input_variables=["entity", "news"])
        chain = prompt | working_llm
        response = chain.invoke({"entity": entity, "news": news_text})
        
        # Extract content from response
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        if "Impact:" in response_text and "Reasoning:" in response_text:
            impact_part, reasoning_part = response_text.split("Reasoning:")
            impact_temp = impact_part.split("Impact:")[1].strip()
            
            # Validate impact category
            valid_impacts = [
                "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏",
                "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
            ]
            if impact_temp in valid_impacts:
                impact = impact_temp
            reasoning = reasoning_part.strip()
            
    except Exception as e:
        st.warning(f"Error in impact estimation: {str(e)}")
    
    return impact, reasoning
   
class QwenSystem:
    def __init__(self):
        """Initialize Qwen 2.5 Coder model"""
        try:
            self.model_name = "Qwen/Qwen2.5-Coder-32B-Instruct"
            
            # Initialize model with auto settings
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            st.success(f"–∑–∞–ø—É—Å—Ç–∏–ª Qwen2.5 model")
            
        except Exception as e:
            st.error(f"–æ—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Qwen2.5: {str(e)}")
            raise

    def invoke(self, messages):
        """Process messages using Qwen's chat template"""
        try:
            # Prepare messages with system prompt
            chat_messages = [
                {"role": "system", "content": "You are wise financial analyst. You are a helpful assistant."}
            ]
            chat_messages.extend(messages)
            
            # Apply chat template
            text = self.tokenizer.apply_chat_template(
                chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Prepare model inputs
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # Generate response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            # Extract new tokens
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            # Decode response
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # Return in ChatOpenAI-compatible format
            return type('Response', (), {'content': response})()
            
        except Exception as e:
            st.warning(f"Qwen generation error: {str(e)}")
            raise


class ProcessingUI:
    def __init__(self):
        if 'control' not in st.session_state:
            st.session_state.control = ProcessControl()
        if 'negative_container' not in st.session_state:
            st.session_state.negative_container = st.empty()
        if 'events_container' not in st.session_state:
            st.session_state.events_container = st.empty()
            
        # Create control buttons
        col1, col2 = st.columns(2)
        with col1:
            if st.button("‚è∏Ô∏è –ü–∞—É–∑–∞/–í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å" if not st.session_state.control.is_paused() else "‚ñ∂Ô∏è –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å", key="pause_button"):
                if st.session_state.control.is_paused():
                    st.session_state.control.resume()
                else:
                    st.session_state.control.pause()
                    
        with col2:
            if st.button("‚èπÔ∏è –°—Ç–æ–ø –∏ –≤—Å—ë", key="stop_button"):
                st.session_state.control.stop()
                
        self.progress_bar = st.progress(0)
        self.status = st.empty()
        
    def update_progress(self, current, total):
        progress = current / total
        self.progress_bar.progress(progress)
        self.status.text(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {current} –∏–∑ {total} —Å–æ–æ–±—â–µ–Ω–∏–π...")
        
    def show_negative(self, entity, headline, analysis, impact=None):
        with st.session_state.negative_container:
            st.markdown(f"""
            <div style='background-color: #ffebee; padding: 10px; border-radius: 5px; margin: 5px 0;'>
                <strong style='color: #d32f2f;'>‚ö†Ô∏è Negative Alert:</strong><br>
                <strong>Entity:</strong> {entity}<br>
                <strong>News:</strong> {headline}<br>
                <strong>Analysis:</strong> {analysis}<br>
                {f"<strong>Impact:</strong> {impact}<br>" if impact else ""}
            </div>
            """, unsafe_allow_html=True)
            
    def show_event(self, entity, event_type, headline):
        with st.session_state.events_container:
            st.markdown(f"""
            <div style='background-color: #e3f2fd; padding: 10px; border-radius: 5px; margin: 5px 0;'>
                <strong style='color: #1976d2;'>üîî Event Detected:</strong><br>
                <strong>Entity:</strong> {entity}<br>
                <strong>Type:</strong> {event_type}<br>
                <strong>News:</strong> {headline}
            </div>
            """, unsafe_allow_html=True)

class EventDetectionSystem:
    def __init__(self):
        try:
            # Initialize models with specific labels
            self.finbert = pipeline(
                "text-classification", 
                model="ProsusAI/finbert",
                return_all_scores=True
            )
            self.business_classifier = pipeline(
                "text-classification", 
                model="yiyanghkust/finbert-tone",
                return_all_scores=True
            )
            st.success("BERT-–º–æ–¥–µ–ª–∏ –∑–∞–ø—É—â–µ–Ω—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ BERT: {str(e)}")
            raise

    def detect_event_type(self, text, entity):
        event_type = "–ù–µ—Ç"
        summary = ""
        
        try:
            # Ensure text is properly formatted
            text = str(text).strip()
            if not text:
                return "–ù–µ—Ç", "Empty text"

            # Get predictions
            finbert_scores = self.finbert(
                text,
                truncation=True,
                max_length=512
            )
            business_scores = self.business_classifier(
                text,
                truncation=True,
                max_length=512
            )
            
            # Get highest scoring predictions
            finbert_pred = max(finbert_scores[0], key=lambda x: x['score'])
            business_pred = max(business_scores[0], key=lambda x: x['score'])
            
            # Map to event types with confidence threshold
            confidence_threshold = 0.6
            max_confidence = max(finbert_pred['score'], business_pred['score'])
            
            if max_confidence >= confidence_threshold:
                if any(term in text.lower() for term in ['–æ—Ç—á–µ—Ç', '–≤—ã—Ä—É—á–∫–∞', '–ø—Ä–∏–±—ã–ª—å', 'ebitda']):
                    event_type = "–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å"
                    summary = f"–§–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å (confidence: {max_confidence:.2f})"
                elif any(term in text.lower() for term in ['–æ–±–ª–∏–≥–∞—Ü–∏', '–∫—É–ø–æ–Ω', '–¥–µ—Ñ–æ–ª—Ç', '—Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü']):
                    event_type = "–†–¶–ë"
                    summary = f"–°–æ–±—ã—Ç–∏–µ –†–¶–ë (confidence: {max_confidence:.2f})"
                elif any(term in text.lower() for term in ['—Å—É–¥', '–∏—Å–∫', '–∞—Ä–±–∏—Ç—Ä–∞–∂']):
                    event_type = "–°—É–¥"
                    summary = f"–°—É–¥–µ–±–Ω–æ–µ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–æ (confidence: {max_confidence:.2f})"
            
            if event_type != "–ù–µ—Ç":
                summary += f"\n–ö–æ–º–ø–∞–Ω–∏—è: {entity}"
            
            return event_type, summary
            
        except Exception as e:
            st.warning(f"Event detection error: {str(e)}")
            return "–ù–µ—Ç", "Error in event detection"

class TranslationSystem:
    def __init__(self):
        """Initialize translation system using Helsinki NLP model with fallback options"""
        try:
            self.translator = pipeline("translation", model="Helsinki-NLP/opus-mt-ru-en")
            # Initialize fallback translator
            self.fallback_translator = GoogleTranslator(source='ru', target='en')
            self.legacy_translator = LegacyTranslator()
            st.success("–ó–∞–ø—É—Å—Ç–∏–ª —Å–∏—Å—Ç–µ–º—É –ø–µ—Ä–µ–≤–æ–¥–∞")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}")
            raise

    def _split_into_chunks(self, text: str, max_length: int = 450) -> list:
        """Split text into chunks while preserving word boundaries"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for word in words:
            word_length = len(word)
            if current_length + word_length + 1 <= max_length:
                current_chunk.append(word)
                current_length += word_length + 1
            else:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = word_length

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _translate_chunk_with_retries(self, chunk: str, max_retries: int = 3) -> str:
        """Attempt translation with multiple fallback options"""
        if not chunk or not chunk.strip():
            return ""

        for attempt in range(max_retries):
            try:
                # First try Helsinki NLP
                result = self.translator(chunk, max_length=512)
                if result and isinstance(result, list) and len(result) > 0:
                    translated = result[0].get('translation_text')
                    if translated and isinstance(translated, str):
                        return translated

                # First fallback: Google Translator
                translated = self.fallback_translator.translate(chunk)
                if translated and isinstance(translated, str):
                    return translated

                # Second fallback: Legacy Google Translator
                translated = self.legacy_translator.translate(chunk, src='ru', dest='en').text
                if translated and isinstance(translated, str):
                    return translated

            except Exception as e:
                if attempt == max_retries - 1:
                    st.warning(f"–ü–æ–ø—Ä–æ–±–æ–≤–∞–ª –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ {max_retries} —Ä–∞–∑, –Ω–µ –ø—Ä–µ—É—Å–ø–µ–ª: {str(e)}")
                time.sleep(1 * (attempt + 1))  # Exponential backoff

        return chunk  # Return original text if all translation attempts fail

    def translate_text(self, text: str) -> str:
        """Translate text with robust error handling and validation"""
        # Input validation
        if pd.isna(text) or not isinstance(text, str):
            return str(text) if pd.notna(text) else ""

        text = str(text).strip()
        if not text:
            return ""

        try:
            # Split into manageable chunks
            chunks = self._split_into_chunks(text)
            translated_chunks = []

            # Process each chunk with validation
            for chunk in chunks:
                if not chunk.strip():
                    continue

                translated_chunk = self._translate_chunk_with_retries(chunk)
                if translated_chunk:  # Only add non-empty translations
                    translated_chunks.append(translated_chunk)
                time.sleep(0.1)  # Rate limiting

            # Final validation of results
            if not translated_chunks:
                return text  # Return original if no translations succeeded

            result = ' '.join(translated_chunks)
            return result if result.strip() else text

        except Exception as e:
            st.warning(f"Translation error: {str(e)}")
            return text  # Return original text on error



def process_file(uploaded_file, model_choice, translation_method=None):
    df = None
    try:
        # Initialize UI and control systems
        ui = ProcessingUI()
        translator = TranslationSystem()
        event_detector = EventDetectionSystem()
        
        # Load and prepare data
        df = pd.read_excel(uploaded_file, sheet_name='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏')
        llm = init_langchain_llm(model_choice)
        
        # Initialize Groq for impact estimation
        groq_llm = ensure_groq_llm()
        if groq_llm is None:
            st.warning("Failed to initialize Groq LLM for impact estimation. Using fallback model.")
        
        # Prepare dataframe
        text_columns = ['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']
        for col in text_columns:
            df[col] = df[col].fillna('').astype(str).apply(lambda x: x.strip())
            
        # Initialize required columns
        df['Translated'] = ''
        df['Sentiment'] = ''
        df['Impact'] = ''
        df['Reasoning'] = ''
        df['Event_Type'] = ''
        df['Event_Summary'] = ''
        
        # Deduplication
        original_count = len(df)
        df = df.groupby('–û–±—ä–µ–∫—Ç', group_keys=False).apply(
            lambda x: fuzzy_deduplicate(x, '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 65)
        ).reset_index(drop=True)
        st.write(f"–ò–∑ {original_count} —Å–æ–æ–±—â–µ–Ω–∏–π —É–¥–∞–ª–µ–Ω–æ {original_count - len(df)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.")
        
        # Process rows
        total_rows = len(df)
        processed_rows = 0
        
        for idx, row in df.iterrows():
            # Check for stop/pause
            if st.session_state.control.is_stopped():
                st.warning("–û–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏")
                break
                
            st.session_state.control.wait_if_paused()
            if st.session_state.control.is_paused():
                st.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –ø–∞—É–∑–µ. –ú–æ–∂–Ω–æ –≤–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å.")
                continue
                
            try:
                # Translation
                translated_text = translator.translate_text(row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
                df.at[idx, 'Translated'] = translated_text
                
                # Sentiment analysis
                sentiment = analyze_sentiment(translated_text)
                df.at[idx, 'Sentiment'] = sentiment
                
                # Event detection using BERT
                event_type, event_summary = event_detector.detect_event_type(
                    translated_text,
                    row['–û–±—ä–µ–∫—Ç']
                )
                df.at[idx, 'Event_Type'] = event_type
                df.at[idx, 'Event_Summary'] = event_summary
                
                # Show events in real-time
                if event_type != "–ù–µ—Ç":
                    ui.show_event(
                        row['–û–±—ä–µ–∫—Ç'],
                        event_type,
                        row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']
                    )
                
                # Handle negative sentiment
                if sentiment == "Negative":
                    try:
                        impact, reasoning = estimate_impact(
                            groq_llm if groq_llm is not None else llm,
                            translated_text,
                            row['–û–±—ä–µ–∫—Ç']
                        )
                    except Exception as e:
                        impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
                        reasoning = "Error in impact estimation"
                        if 'rate limit' in str(e).lower():
                            st.warning("–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∏—Å—á–µ—Ä–ø–∞–ª—Å—è. –ò–¥—É –Ω–∞ fallback.")
                    
                    df.at[idx, 'Impact'] = impact
                    df.at[idx, 'Reasoning'] = reasoning
                    
                    # Show negative alert in real-time
                    ui.show_negative(
                        row['–û–±—ä–µ–∫—Ç'],
                        row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'],
                        reasoning,
                        impact
                    )
                
                # Update progress
                processed_rows += 1
                ui.update_progress(processed_rows, total_rows)
                
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä—è–¥–∞ {idx + 1}: {str(e)}")
                continue
            
            time.sleep(0.1)
        
        # Handle stopped processing
        if st.session_state.control.is_stopped() and len(df) > 0:
            st.warning("–û–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏. –ü–æ–∫–∞–∑—ã–≤–∞—é —á–∞—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")
            if st.button("–°–∫–∞—á–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"):
                output = create_output_file(df, uploaded_file, llm)
                st.download_button(
                    label="üìä –°–∫–∞—á–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
                    data=output,
                    file_name="partial_analysis.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
        
        return df
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        return None

def translate_reasoning_to_russian(llm, text):
    template = """
    Translate this English explanation to Russian, maintaining a formal business style:
    "{text}"
    
    Your response should contain only the Russian translation.
    """
    prompt = PromptTemplate(template=template, input_variables=["text"])
    chain = prompt | llm | RunnablePassthrough()
    response = chain.invoke({"text": text})
    
    # Handle different response types
    if hasattr(response, 'content'):
        return response.content.strip()
    elif isinstance(response, str):
        return response.strip()
    else:
        return str(response).strip()
    

def create_download_section(excel_data, pdf_data):
    st.markdown("""
        <div class="download-container">
            <div class="download-header">üì• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:</div>
        </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    
    with col1:
        if excel_data is not None:
            st.download_button(
                label="üìä –°–∫–∞—á–∞—Ç—å Excel –æ—Ç—á–µ—Ç",
                data=excel_data,
                file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="excel_download"
            )
        else:
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞")
    



def display_sentiment_results(row, sentiment, impact=None, reasoning=None):
    if sentiment == "Negative":
        st.markdown(f"""
            <div style='color: red; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            {"–≠—Ñ—Ñ–µ–∫—Ç: " + impact + "<br>" if impact else ""}
            {"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: " + reasoning + "<br>" if reasoning else ""}
            </div>
            """, unsafe_allow_html=True)
    elif sentiment == "Positive":
        st.markdown(f"""
            <div style='color: green; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.write(f"–û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}")
        st.write(f"–ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}")
        st.write(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}")
    
    st.write("---")




    
# Initialize sentiment analyzers
finbert = pipeline("sentiment-analysis", model="ProsusAI/finbert")
roberta = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment")
finbert_tone = pipeline("sentiment-analysis", model="yiyanghkust/finbert-tone")


def get_mapped_sentiment(result):
    label = result['label'].lower()
    if label in ["positive", "label_2", "pos", "pos_label"]:
        return "Positive"
    elif label in ["negative", "label_0", "neg", "neg_label"]:
        return "Negative"
    return "Neutral"



def analyze_sentiment(text):
    try:
        finbert_result = get_mapped_sentiment(
            finbert(text, truncation=True, max_length=512)[0]
        )
        roberta_result = get_mapped_sentiment(
            roberta(text, truncation=True, max_length=512)[0]
        )
        finbert_tone_result = get_mapped_sentiment(
            finbert_tone(text, truncation=True, max_length=512)[0]
        )
        
        # Count occurrences of each sentiment
        sentiments = [finbert_result, roberta_result, finbert_tone_result]
        sentiment_counts = {s: sentiments.count(s) for s in set(sentiments)}
        
        # Return sentiment if at least two models agree
        for sentiment, count in sentiment_counts.items():
            if count >= 2:
                return sentiment
                
        # Default to Neutral if no agreement
        return "Neutral"
        
    except Exception as e:
        st.warning(f"Sentiment analysis error: {str(e)}")
        return "Neutral"


def fuzzy_deduplicate(df, column, threshold=50):
    seen_texts = []
    indices_to_keep = []
    for i, text in enumerate(df[column]):
        if pd.isna(text):
            indices_to_keep.append(i)
            continue
        text = str(text)
        if not seen_texts or all(fuzz.ratio(text, seen) < threshold for seen in seen_texts):
            seen_texts.append(text)
            indices_to_keep.append(i)
    return df.iloc[indices_to_keep]


def init_langchain_llm(model_choice):
    try:
        if model_choice == "Qwen2.5-Coder":
            st.info("Loading Qwen2.5-Coder model. —Ç–æ–ª—å–∫–æ GPU!")
            return QwenSystem()
            
        elif model_choice == "Groq (llama-3.1-70b)":
            if 'groq_key' not in st.secrets:
                st.error("Groq API key not found in secrets. Please add it with the key 'groq_key'.")
                st.stop()
                
            return ChatOpenAI(
                base_url="https://api.groq.com/openai/v1",
                model="llama-3.1-70b-versatile",
                openai_api_key=st.secrets['groq_key'],
                temperature=0.0
            )
            
        elif model_choice == "ChatGPT-4-mini":
            if 'openai_key' not in st.secrets:
                st.error("OpenAI API key not found in secrets. Please add it with the key 'openai_key'.")
                st.stop()
                
            return ChatOpenAI(
                model="gpt-4",
                openai_api_key=st.secrets['openai_key'],
                temperature=0.0
            )
            
        elif model_choice == "Local-MT5":
            return FallbackLLMSystem()
            
    except Exception as e:
        st.error(f"Error initializing the LLM: {str(e)}")
        st.stop()


def estimate_impact(llm, news_text, entity):
    template = """
    Analyze the following news piece about the entity "{entity}" and estimate its monetary impact in Russian rubles for this entity in the next 6 months.
    
    If precise monetary estimate is not possible, categorize the impact as one of the following:
    1. "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" 
    2. "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤"
    3. "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤"
    4. "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏"
    5. "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"

    Provide brief reasoning (maximum 100 words).

    News: {news}

    Your response should be in the following format:
    Impact: [Your estimate or category]
    Reasoning: [Your reasoning]
    """
    prompt = PromptTemplate(template=template, input_variables=["entity", "news"])
    chain = prompt | llm
    response = chain.invoke({"entity": entity, "news": news_text})
    
    impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
    reasoning = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    
    # Extract content from response
    response_text = response.content if hasattr(response, 'content') else str(response)
    
    try:
        if "Impact:" in response_text and "Reasoning:" in response_text:
            impact_part, reasoning_part = response_text.split("Reasoning:")
            impact = impact_part.split("Impact:")[1].strip()
            reasoning = reasoning_part.strip()
    except Exception as e:
        st.error(f"Error parsing LLM response: {str(e)}")
    
    return impact, reasoning

def format_elapsed_time(seconds):
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    time_parts = []
    if hours > 0:
        time_parts.append(f"{hours} —á–∞—Å{'–æ–≤' if hours != 1 else ''}")
    if minutes > 0:
        time_parts.append(f"{minutes} –º–∏–Ω—É—Ç{'' if minutes == 1 else '—ã' if 2 <= minutes <= 4 else ''}")
    if seconds > 0 or not time_parts:
        time_parts.append(f"{seconds} —Å–µ–∫—É–Ω–¥{'–∞' if seconds == 1 else '—ã' if 2 <= seconds <= 4 else ''}")
    
    return " ".join(time_parts)

def generate_sentiment_visualization(df):
    negative_df = df[df['Sentiment'] == 'Negative']
    
    if negative_df.empty:
        st.warning("–ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—ä–µ–∫—Ç–∞–º.")
        entity_counts = df['–û–±—ä–µ–∫—Ç'].value_counts()
    else:
        entity_counts = negative_df['–û–±—ä–µ–∫—Ç'].value_counts()
    
    if len(entity_counts) == 0:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.")
        return None
    
    fig, ax = plt.subplots(figsize=(12, max(6, len(entity_counts) * 0.5)))
    entity_counts.plot(kind='barh', ax=ax)
    ax.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ –æ–±—ä–µ–∫—Ç–∞–º')
    ax.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')
    plt.tight_layout()
    return fig

def create_analysis_data(df):
    analysis_data = []
    for _, row in df.iterrows():
        if row['Sentiment'] == 'Negative':
            analysis_data.append([
                row['–û–±—ä–µ–∫—Ç'], 
                row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'], 
                '–†–ò–°–ö –£–ë–´–¢–ö–ê', 
                row['Impact'],
                row['Reasoning'],
                row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']
            ])
    return pd.DataFrame(analysis_data, columns=[
        '–û–±—ä–µ–∫—Ç', 
        '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 
        '–ü—Ä–∏–∑–Ω–∞–∫', 
        '–û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è',
        '–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ',
        '–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'
    ])

def create_output_file(df, uploaded_file, llm):
    wb = load_workbook("sample_file.xlsx")
    
    try:
        # Update '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' sheet with events
        ws = wb['–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥']
        row_idx = 4
        for _, row in df.iterrows():
            if row['Event_Type'] != '–ù–µ—Ç':
                ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])  # Column E
                ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])  # Column F
                ws.cell(row=row_idx, column=7, value=row['Event_Type'])  # Column G
                ws.cell(row=row_idx, column=8, value=row['Event_Summary'])  # Column H
                ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])  # Column I
                row_idx += 1
                   
        # Sort entities by number of negative publications
        entity_stats = pd.DataFrame({
            '–û–±—ä–µ–∫—Ç': df['–û–±—ä–µ–∫—Ç'].unique(),
            '–í—Å–µ–≥–æ': df.groupby('–û–±—ä–µ–∫—Ç').size(),
            '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ': df[df['Sentiment'] == 'Negative'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int),
            '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ': df[df['Sentiment'] == 'Positive'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int)
        }).sort_values('–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ', ascending=False)
        
        # Calculate most negative impact for each entity
        entity_impacts = {}
        for entity in df['–û–±—ä–µ–∫—Ç'].unique():
            entity_df = df[df['–û–±—ä–µ–∫—Ç'] == entity]
            negative_impacts = entity_df[entity_df['Sentiment'] == 'Negative']['Impact']
            entity_impacts[entity] = negative_impacts.iloc[0] if len(negative_impacts) > 0 else '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç'
        
        # Update '–°–≤–æ–¥–∫–∞' sheet
        ws = wb['–°–≤–æ–¥–∫–∞']
        for idx, (entity, row) in enumerate(entity_stats.iterrows(), start=4):
            ws.cell(row=idx, column=5, value=entity)  # Column E
            ws.cell(row=idx, column=6, value=row['–í—Å–µ–≥–æ'])  # Column F
            ws.cell(row=idx, column=7, value=row['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ'])  # Column G
            ws.cell(row=idx, column=8, value=row['–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ'])  # Column H
            ws.cell(row=idx, column=9, value=entity_impacts[entity])  # Column I
        
        # Update '–ó–Ω–∞—á–∏–º—ã–µ' sheet
        ws = wb['–ó–Ω–∞—á–∏–º—ã–µ']
        row_idx = 3
        for _, row in df.iterrows():
            if row['Sentiment'] in ['Negative', 'Positive']:
                ws.cell(row=row_idx, column=3, value=row['–û–±—ä–µ–∫—Ç'])  # Column C
                ws.cell(row=row_idx, column=4, value='—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ')   # Column D
                ws.cell(row=row_idx, column=5, value=row['Sentiment']) # Column E
                ws.cell(row=row_idx, column=6, value=row['Impact'])   # Column F
                ws.cell(row=row_idx, column=7, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']) # Column G
                ws.cell(row=row_idx, column=8, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']) # Column H
                row_idx += 1
        
        # Copy '–ü—É–±–ª–∏–∫–∞—Ü–∏–∏' sheet
        original_df = pd.read_excel(uploaded_file, sheet_name='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏')
        ws = wb['–ü—É–±–ª–∏–∫–∞—Ü–∏–∏']
        for r_idx, row in enumerate(dataframe_to_rows(original_df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)
        
        # Update '–ê–Ω–∞–ª–∏–∑' sheet
        ws = wb['–ê–Ω–∞–ª–∏–∑']
        row_idx = 4
        for _, row in df[df['Sentiment'] == 'Negative'].iterrows():
            ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])  # Column E
            ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])  # Column F
            ws.cell(row=row_idx, column=7, value="–†–∏—Å–∫ —É–±—ã—Ç–∫–∞")  # Column G
            
            # Translate reasoning if it exists
            if pd.notna(row['Reasoning']):
                translated_reasoning = translate_reasoning_to_russian(llm, row['Reasoning'])
                ws.cell(row=row_idx, column=8, value=translated_reasoning)  # Column H
            
            ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])  # Column I
            row_idx += 1
        
        # Update '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' sheet
        tech_df = df[['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 'Translated', 'Sentiment', 'Impact', 'Reasoning']]
        if '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' not in wb.sheetnames:
            wb.create_sheet('–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ')
        ws = wb['–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']
        for r_idx, row in enumerate(dataframe_to_rows(tech_df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)
    
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {str(e)}")
    
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    return output

def main():
    st.set_page_config(layout="wide")
    
    with st.sidebar:
        st.title("::: AI-–∞–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (v.3.57):::")
        st.subheader("–ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –°–ö–ê–ù-–ò–ù–¢–ï–†–§–ê–ö–°")
        
        model_choice = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
            ["Local-MT5", "Qwen2.5-Coder", "Groq (llama-3.1-70b)", "ChatGPT-4-mini"],
            key="model_selector",
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
        )
        
        uploaded_file = st.file_uploader(
            "–í—ã–±–∏—Ä–∞–π—Ç–µ Excel-—Ñ–∞–π–ª",
            type="xlsx",
            key="file_uploader"
        )
        
        st.markdown(
            """
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:  
            - –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π **BERT**
            - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (**LLM**)
            - –§—Ä–µ–π–º–≤–æ—Ä–∫ **LangChain** –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏
            """,
            unsafe_allow_html=True
        )

    # Main content area
    st.title("–ê–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    
    # Initialize session state
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
        
    # Create display areas
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Area for real-time updates
        st.subheader("–ß—Ç–æ –Ω–∞–π–¥–µ–Ω–æ, —Å–æ–æ–±—â–∞—é:")
        st.markdown("""
            <style>
            .stProgress .st-bo {
                background-color: #f0f2f6;
            }
            .negative-alert {
                background-color: #ffebee;
                border-left: 5px solid #f44336;
                padding: 10px;
                margin: 5px 0;
            }
            .event-alert {
                background-color: #e3f2fd;
                border-left: 5px solid #2196f3;
                padding: 10px;
                margin: 5px 0;
            }
            </style>
        """, unsafe_allow_html=True)
        
    with col2:
        # Area for statistics
        st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        if st.session_state.processed_df is not None:
            st.metric("–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π", len(st.session_state.processed_df))
            st.metric("–ò–∑ –Ω–∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö", 
                len(st.session_state.processed_df[
                    st.session_state.processed_df['Sentiment'] == 'Negative'
                ])
            )
            st.metric("–°–æ–±—ã—Ç–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", 
                len(st.session_state.processed_df[
                    st.session_state.processed_df['Event_Type'] != '–ù–µ—Ç'
                ])
            )
    
    if uploaded_file is not None and st.session_state.processed_df is None:
        start_time = time.time()
        
        try:
            st.session_state.processed_df = process_file(
                uploaded_file,
                model_choice,
                translation_method='auto'
            )
            
            if st.session_state.processed_df is not None:
                end_time = time.time()
                elapsed_time = format_elapsed_time(end_time - start_time)
                
                # Show results
                st.subheader("–ò—Ç–æ–≥–æ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º")
                
                # Display statistics
                stats_cols = st.columns(4)
                with stats_cols[0]:
                    st.metric("–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ", len(st.session_state.processed_df))
                with stats_cols[1]:
                    st.metric("–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö", 
                        len(st.session_state.processed_df[
                            st.session_state.processed_df['Sentiment'] == 'Negative'
                        ])
                    )
                with stats_cols[2]:
                    st.metric("–°–æ–±—ã—Ç–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", 
                        len(st.session_state.processed_df[
                            st.session_state.processed_df['Event_Type'] != '–ù–µ—Ç'
                        ])
                    )
                with stats_cols[3]:
                    st.metric("–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–æ", elapsed_time)
                
                # Show data previews
                with st.expander("üìä –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö", expanded=True):
                    preview_cols = ['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'Sentiment', 'Event_Type']
                    st.dataframe(
                        st.session_state.processed_df[preview_cols],
                        use_container_width=True
                    )
                
                # Create downloadable report
                output = create_output_file(
                    st.session_state.processed_df,
                    uploaded_file,
                    init_langchain_llm(model_choice)
                )
                
                st.download_button(
                    label="üì• –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç - –∑–∞–≥—Ä—É–∑–∏—Ç—å",
                    data=output,
                    file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key='download_button'
                )
                
        except Exception as e:
            st.error(f"–û—à–∏–±–æ—á–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
            st.session_state.processed_df = None


if __name__ == "__main__":
    main()