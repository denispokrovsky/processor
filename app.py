import streamlit as st
import pandas as pd
import time
import matplotlib.pyplot as plt
from openpyxl.utils.dataframe import dataframe_to_rows
import io
from rapidfuzz import fuzz
import os
from openpyxl import load_workbook
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from transformers import pipeline
from io import StringIO, BytesIO
import sys
import contextlib
from langchain_openai import ChatOpenAI  # Updated import
import pdfkit
from jinja2 import Template
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Optional
from deep_translator import GoogleTranslator as DeepGoogleTranslator
from googletrans import Translator as LegacyTranslator

class TranslationSystem:
    def __init__(self, method='auto', llm=None, batch_size=10):
        """
        Initialize translation system with multiple fallback options.
        
        Args:
            method (str): 'auto', 'deep-google', or 'llm'
            llm: LangChain LLM instance (required if method is 'llm')
            batch_size (int): Number of texts to process in each batch
        """
        self.method = method
        self.llm = llm
        self.batch_size = batch_size
        self.rate_limiter = RateLimitHandler()
        self.translator = None
        self._initialize_translator()
        
    def _initialize_translator(self):
        """
        Initialize translator with fallback options.
        """
        if self.method == 'llm':
            if not self.llm:
                raise Exception("LLM must be provided when using 'llm' method")
            return
            
        try:
            # Try deep-translator first (more stable)
            self.translator = DeepGoogleTranslator()
            self.method = 'deep-google'
            # Test translation
            test_result = self.translator.translate(text='test', source='en', target='ru')
            if not test_result:
                raise Exception("Deep translator test failed")
                
        except Exception as deep_e:
            st.warning(f"Deep-translator initialization failed: {str(deep_e)}")
            
            if self.method != 'llm' and self.llm:
                st.info("Falling back to LLM translation")
                self.method = 'llm'
            else:
                raise Exception("No translation method available")

    def translate_batch(self, texts, src='ru', dest='en'):
        """
        Translate a batch of texts with fallback options.
        """
        translations = []
        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]
            batch_translations = []
            
            for text in batch:
                try:
                    translation = self.rate_limiter.execute_with_retry(
                        self._translate_single_text,
                        text,
                        src,
                        dest
                    )
                    batch_translations.append(translation)
                except Exception as e:
                    st.warning(f"Translation error: {str(e)}. Using original text.")
                    batch_translations.append(text)
                    
                    # If deep-google fails, try falling back to LLM
                    if self.method == 'deep-google' and self.llm:
                        try:
                            st.info("Attempting LLM translation fallback...")
                            self.method = 'llm'
                            translation = self._translate_single_text(text, src, dest)
                            batch_translations[-1] = translation  # Replace original text with translation
                        except Exception as llm_e:
                            st.warning(f"LLM fallback failed: {str(llm_e)}")
                            
            translations.extend(batch_translations)
            time.sleep(1)  # Small delay between batches
            
        return translations
    
    def _translate_single_text(self, text, src='ru', dest='en'):
        """
        Translate a single text with appropriate method.
        """
        if pd.isna(text) or not isinstance(text, str) or not text.strip():
            return text
            
        text = text.strip()
        
        if self.method == 'llm':
            return self._translate_with_llm(text, src, dest)
        elif self.method == 'deep-google':
            return self._translate_with_deep_google(text, src, dest)
        else:
            raise Exception(f"Unsupported translation method: {self.method}")
            
    def _translate_with_deep_google(self, text, src='ru', dest='en'):
        """
        Translate using deep-translator's Google Translate.
        """
        try:
            # deep-translator uses different language codes
            src = 'auto' if src == 'auto' else src.lower()
            dest = dest.lower()
            
            # Split long texts (deep-translator has a character limit)
            max_length = 5000
            if len(text) > max_length:
                chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
                translated_chunks = []
                for chunk in chunks:
                    translated_chunk = self.translator.translate(
                        text=chunk,
                        source=src,
                        target=dest
                    )
                    translated_chunks.append(translated_chunk)
                return ' '.join(translated_chunks)
            else:
                return self.translator.translate(
                    text=text,
                    source=src,
                    target=dest
                )
                
        except Exception as e:
            raise Exception(f"Deep-translator error: {str(e)}")
            
    def _translate_with_llm(self, text, src='ru', dest='en'):
        """
        Translate using LangChain LLM.
        """
        if not self.llm:
            raise Exception("LLM not initialized for translation")
            
        messages = [
            {"role": "system", "content": "You are a translator. Translate the given text accurately and concisely."},
            {"role": "user", "content": f"Translate this text from {src} to {dest}: {text}"}
        ]
        
        response = self.llm.invoke(messages)
        return response.content.strip() if hasattr(response, 'content') else str(response).strip()

def init_translation_system(model_choice, translation_method='auto'):
    """
    Initialize translation system with appropriate configuration.
    """
    llm = init_langchain_llm(model_choice) if translation_method != 'deep-google' else None
    
    try:
        translator = TranslationSystem(
            method=translation_method,
            llm=llm,
            batch_size=5
        )
        return translator
    except Exception as e:
        st.error(f"Failed to initialize translation system: {str(e)}")
        raise

def process_file(uploaded_file, model_choice, translation_method='googletrans'):
    df = None
    try:
        df = pd.read_excel(uploaded_file, sheet_name='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏')
        llm = init_langchain_llm(model_choice)
        
        # In your process_file function:
        translator = init_translation_system(
            model_choice=model_choice,
            translation_method='auto'  # Will try deep-translator first, then fal
            l back to LLM if needed
        )
        # Validate required columns
        required_columns = ['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            st.error(f"Error: The following required columns are missing: {', '.join(missing_columns)}")
            return df if df is not None else None
        
        # Deduplication
        original_news_count = len(df)
        df = df.groupby('–û–±—ä–µ–∫—Ç', group_keys=False).apply(
            lambda x: fuzzy_deduplicate(x, '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 65)
        ).reset_index(drop=True)
    
        remaining_news_count = len(df)
        duplicates_removed = original_news_count - remaining_news_count
        st.write(f"–ò–∑ {original_news_count} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —É–¥–∞–ª–µ–Ω—ã {duplicates_removed} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö. –û—Å—Ç–∞–ª–æ—Å—å {remaining_news_count}.")

        # Initialize progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Initialize new columns
        df['Translated'] = ''
        df['Sentiment'] = ''
        df['Impact'] = ''
        df['Reasoning'] = ''
        df['Event_Type'] = ''
        df['Event_Summary'] = ''
        
        # Process each news item
        for index, row in df.iterrows():
            try:
                # Translate and analyze sentiment
                translated_text = translator.translate_text(row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
                df.at[index, 'Translated'] = translated_text
                
                sentiment = analyze_sentiment(translated_text)
                df.at[index, 'Sentiment'] = sentiment
                
                # Detect events
                event_type, event_summary = detect_events(llm, row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'], row['–û–±—ä–µ–∫—Ç'])
                df.at[index, 'Event_Type'] = event_type
                df.at[index, 'Event_Summary'] = event_summary
                
                if sentiment == "Negative":
                    impact, reasoning = estimate_impact(llm, translated_text, row['–û–±—ä–µ–∫—Ç'])
                    df.at[index, 'Impact'] = impact
                    df.at[index, 'Reasoning'] = reasoning
                
                # Update progress
                progress = (index + 1) / len(df)
                progress_bar.progress(progress)
                status_text.text(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {index + 1} –∏–∑ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π")
                
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–∏ {index + 1}: {str(e)}")
                continue
        
        return df
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        return df if df is not None else None


def translate_reasoning_to_russian(llm, text):
    template = """
    Translate this English explanation to Russian, maintaining a formal business style:
    "{text}"
    
    Your response should contain only the Russian translation.
    """
    prompt = PromptTemplate(template=template, input_variables=["text"])
    chain = prompt | llm | RunnablePassthrough()
    response = chain.invoke({"text": text})
    
    # Handle different response types
    if hasattr(response, 'content'):
        return response.content.strip()
    elif isinstance(response, str):
        return response.strip()
    else:
        return str(response).strip()
    

def create_download_section(excel_data, pdf_data):
    st.markdown("""
        <div class="download-container">
            <div class="download-header">üì• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:</div>
        </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    
    with col1:
        if excel_data is not None:
            st.download_button(
                label="üìä –°–∫–∞—á–∞—Ç—å Excel –æ—Ç—á–µ—Ç",
                data=excel_data,
                file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="excel_download"
            )
        else:
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞")
    



def display_sentiment_results(row, sentiment, impact=None, reasoning=None):
    if sentiment == "Negative":
        st.markdown(f"""
            <div style='color: red; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            {"–≠—Ñ—Ñ–µ–∫—Ç: " + impact + "<br>" if impact else ""}
            {"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: " + reasoning + "<br>" if reasoning else ""}
            </div>
            """, unsafe_allow_html=True)
    elif sentiment == "Positive":
        st.markdown(f"""
            <div style='color: green; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.write(f"–û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}")
        st.write(f"–ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}")
        st.write(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}")
    
    st.write("---")




    
# Initialize sentiment analyzers
finbert = pipeline("sentiment-analysis", model="ProsusAI/finbert")
roberta = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment")
finbert_tone = pipeline("sentiment-analysis", model="yiyanghkust/finbert-tone")


def get_mapped_sentiment(result):
    label = result['label'].lower()
    if label in ["positive", "label_2", "pos", "pos_label"]:
        return "Positive"
    elif label in ["negative", "label_0", "neg", "neg_label"]:
        return "Negative"
    return "Neutral"



def analyze_sentiment(text):
    finbert_result = get_mapped_sentiment(finbert(text, truncation=True, max_length=512)[0])
    roberta_result = get_mapped_sentiment(roberta(text, truncation=True, max_length=512)[0])
    finbert_tone_result = get_mapped_sentiment(finbert_tone(text, truncation=True, max_length=512)[0])
    
    # Consider sentiment negative if any model says it's negative
    if any(result == "Negative" for result in [finbert_result, roberta_result, finbert_tone_result]):
        return "Negative"
    elif all(result == "Positive" for result in [finbert_result, roberta_result, finbert_tone_result]):
        return "Positive"
    return "Neutral"

def analyze_sentiment(text):
    finbert_result = get_mapped_sentiment(finbert(text, truncation=True, max_length=512)[0])
    roberta_result = get_mapped_sentiment(roberta(text, truncation=True, max_length=512)[0])
    finbert_tone_result = get_mapped_sentiment(finbert_tone(text, truncation=True, max_length=512)[0])
    
    # Count occurrences of each sentiment
    sentiments = [finbert_result, roberta_result, finbert_tone_result]
    sentiment_counts = {s: sentiments.count(s) for s in set(sentiments)}
    
    # Return sentiment if at least two models agree, otherwise return Neutral
    for sentiment, count in sentiment_counts.items():
        if count >= 2:
            return sentiment
    return "Neutral"


def detect_events(llm, text, entity):
    template = """
    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –æ –∫–æ–º–ø–∞–Ω–∏–∏ "{entity}" –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–±—ã—Ç–∏–π:
    1. –ü—É–±–ª–∏–∫–∞—Ü–∏—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏ –∏ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–≤—ã—Ä—É—á–∫–∞, –ø—Ä–∏–±—ã–ª—å, EBITDA)
    2. –°–æ–±—ã—Ç–∏—è –Ω–∞ —Ä—ã–Ω–∫–µ —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥ (–ø–æ–≥–∞—à–µ–Ω–∏–µ –æ–±–ª–∏–≥–∞—Ü–∏–π, –≤—ã–ø–ª–∞—Ç–∞/–Ω–µ–≤—ã–ø–ª–∞—Ç–∞ –∫—É–ø–æ–Ω–∞, –¥–µ—Ñ–æ–ª—Ç, —Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏—è)
    3. –°—É–¥–µ–±–Ω—ã–µ –∏—Å–∫–∏ –∏–ª–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ—Ç–∏–≤ –∫–æ–º–ø–∞–Ω–∏–∏, –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤, –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤

    –ù–æ–≤–æ—Å—Ç—å: {text}

    –û—Ç–≤–µ—Ç—å—Ç–µ –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
    –¢–∏–ø: ["–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å" –∏–ª–∏ "–†–¶–ë" –∏–ª–∏ "–°—É–¥" –∏–ª–∏ "–ù–µ—Ç"]
    –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: [–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –Ω–µ –±–æ–ª–µ–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π]
    """
    
    prompt = PromptTemplate(template=template, input_variables=["entity", "text"])
    chain = prompt | llm
    response = chain.invoke({"entity": entity, "text": text})
    
    event_type = "–ù–µ—Ç"
    summary = ""
    
    try:
        response_text = response.content if hasattr(response, 'content') else str(response)
        if "–¢–∏–ø:" in response_text and "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:" in response_text:
            type_part, summary_part = response_text.split("–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:")
            event_type = type_part.split("–¢–∏–ø:")[1].strip()
            summary = summary_part.strip()
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–æ–±—ã—Ç–∏–π: {str(e)}")
    
    return event_type, summary

def fuzzy_deduplicate(df, column, threshold=50):
    seen_texts = []
    indices_to_keep = []
    for i, text in enumerate(df[column]):
        if pd.isna(text):
            indices_to_keep.append(i)
            continue
        text = str(text)
        if not seen_texts or all(fuzz.ratio(text, seen) < threshold for seen in seen_texts):
            seen_texts.append(text)
            indices_to_keep.append(i)
    return df.iloc[indices_to_keep]


def init_langchain_llm(model_choice):
    try:
        if model_choice == "Groq (llama-3.1-70b)":
            if 'groq_key' not in st.secrets:
                st.error("Groq API key not found in secrets. Please add it with the key 'groq_key'.")
                st.stop()
                
            return ChatOpenAI(
                base_url="https://api.groq.com/openai/v1",
                model="llama-3.1-70b-versatile",
                openai_api_key=st.secrets['groq_key'],
                temperature=0.0
            )
            
        elif model_choice == "ChatGPT-4-mini":
            if 'openai_key' not in st.secrets:
                st.error("OpenAI API key not found in secrets. Please add it with the key 'openai_key'.")
                st.stop()
                
            return ChatOpenAI(
                model="gpt-4",
                openai_api_key=st.secrets['openai_key'],
                temperature=0.0
            )
            
        else:  # Qwen API
            if 'ali_key' not in st.secrets:
                st.error("DashScope API key not found in secrets. Please add it with the key 'dashscope_api_key'.")
                st.stop()
            
            # Using Qwen's API through DashScope
            return ChatOpenAI(
                base_url="https://dashscope.aliyuncs.com/api/v1",
                model="qwen-max",
                openai_api_key=st.secrets['ali_key'],
                temperature=0.0
            )
            
    except Exception as e:
        st.error(f"Error initializing the LLM: {str(e)}")
        st.stop()

def estimate_impact(llm, news_text, entity):
    template = """
    Analyze the following news piece about the entity "{entity}" and estimate its monetary impact in Russian rubles for this entity in the next 6 months.
    
    If precise monetary estimate is not possible, categorize the impact as one of the following:
    1. "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" 
    2. "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤"
    3. "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤"
    4. "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏"
    5. "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"

    Provide brief reasoning (maximum 100 words).

    News: {news}

    Your response should be in the following format:
    Impact: [Your estimate or category]
    Reasoning: [Your reasoning]
    """
    prompt = PromptTemplate(template=template, input_variables=["entity", "news"])
    chain = prompt | llm
    response = chain.invoke({"entity": entity, "news": news_text})
    
    impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
    reasoning = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    
    # Extract content from response
    response_text = response.content if hasattr(response, 'content') else str(response)
    
    try:
        if "Impact:" in response_text and "Reasoning:" in response_text:
            impact_part, reasoning_part = response_text.split("Reasoning:")
            impact = impact_part.split("Impact:")[1].strip()
            reasoning = reasoning_part.strip()
    except Exception as e:
        st.error(f"Error parsing LLM response: {str(e)}")
    
    return impact, reasoning

def format_elapsed_time(seconds):
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    time_parts = []
    if hours > 0:
        time_parts.append(f"{hours} —á–∞—Å{'–æ–≤' if hours != 1 else ''}")
    if minutes > 0:
        time_parts.append(f"{minutes} –º–∏–Ω—É—Ç{'' if minutes == 1 else '—ã' if 2 <= minutes <= 4 else ''}")
    if seconds > 0 or not time_parts:
        time_parts.append(f"{seconds} —Å–µ–∫—É–Ω–¥{'–∞' if seconds == 1 else '—ã' if 2 <= seconds <= 4 else ''}")
    
    return " ".join(time_parts)

def generate_sentiment_visualization(df):
    negative_df = df[df['Sentiment'] == 'Negative']
    
    if negative_df.empty:
        st.warning("–ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—ä–µ–∫—Ç–∞–º.")
        entity_counts = df['–û–±—ä–µ–∫—Ç'].value_counts()
    else:
        entity_counts = negative_df['–û–±—ä–µ–∫—Ç'].value_counts()
    
    if len(entity_counts) == 0:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.")
        return None
    
    fig, ax = plt.subplots(figsize=(12, max(6, len(entity_counts) * 0.5)))
    entity_counts.plot(kind='barh', ax=ax)
    ax.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ –æ–±—ä–µ–∫—Ç–∞–º')
    ax.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')
    plt.tight_layout()
    return fig

def create_analysis_data(df):
    analysis_data = []
    for _, row in df.iterrows():
        if row['Sentiment'] == 'Negative':
            analysis_data.append([
                row['–û–±—ä–µ–∫—Ç'], 
                row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'], 
                '–†–ò–°–ö –£–ë–´–¢–ö–ê', 
                row['Impact'],
                row['Reasoning'],
                row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']
            ])
    return pd.DataFrame(analysis_data, columns=[
        '–û–±—ä–µ–∫—Ç', 
        '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 
        '–ü—Ä–∏–∑–Ω–∞–∫', 
        '–û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è',
        '–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ',
        '–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'
    ])

def create_output_file(df, uploaded_file, llm):
    wb = load_workbook("sample_file.xlsx")
    
    try:
        # Update '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' sheet with events
        ws = wb['–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥']
        row_idx = 4
        for _, row in df.iterrows():
            if row['Event_Type'] != '–ù–µ—Ç':
                ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])  # Column E
                ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])  # Column F
                ws.cell(row=row_idx, column=7, value=row['Event_Type'])  # Column G
                ws.cell(row=row_idx, column=8, value=row['Event_Summary'])  # Column H
                ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])  # Column I
                row_idx += 1
                   
        # Sort entities by number of negative publications
        entity_stats = pd.DataFrame({
            '–û–±—ä–µ–∫—Ç': df['–û–±—ä–µ–∫—Ç'].unique(),
            '–í—Å–µ–≥–æ': df.groupby('–û–±—ä–µ–∫—Ç').size(),
            '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ': df[df['Sentiment'] == 'Negative'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int),
            '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ': df[df['Sentiment'] == 'Positive'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int)
        }).sort_values('–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ', ascending=False)
        
        # Calculate most negative impact for each entity
        entity_impacts = {}
        for entity in df['–û–±—ä–µ–∫—Ç'].unique():
            entity_df = df[df['–û–±—ä–µ–∫—Ç'] == entity]
            negative_impacts = entity_df[entity_df['Sentiment'] == 'Negative']['Impact']
            entity_impacts[entity] = negative_impacts.iloc[0] if len(negative_impacts) > 0 else '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç'
        
        # Update '–°–≤–æ–¥–∫–∞' sheet
        ws = wb['–°–≤–æ–¥–∫–∞']
        for idx, (entity, row) in enumerate(entity_stats.iterrows(), start=4):
            ws.cell(row=idx, column=5, value=entity)  # Column E
            ws.cell(row=idx, column=6, value=row['–í—Å–µ–≥–æ'])  # Column F
            ws.cell(row=idx, column=7, value=row['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ'])  # Column G
            ws.cell(row=idx, column=8, value=row['–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ'])  # Column H
            ws.cell(row=idx, column=9, value=entity_impacts[entity])  # Column I
        
        # Update '–ó–Ω–∞—á–∏–º—ã–µ' sheet
        ws = wb['–ó–Ω–∞—á–∏–º—ã–µ']
        row_idx = 3
        for _, row in df.iterrows():
            if row['Sentiment'] in ['Negative', 'Positive']:
                ws.cell(row=row_idx, column=3, value=row['–û–±—ä–µ–∫—Ç'])  # Column C
                ws.cell(row=row_idx, column=4, value='—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ')   # Column D
                ws.cell(row=row_idx, column=5, value=row['Sentiment']) # Column E
                ws.cell(row=row_idx, column=6, value=row['Impact'])   # Column F
                ws.cell(row=row_idx, column=7, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']) # Column G
                ws.cell(row=row_idx, column=8, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']) # Column H
                row_idx += 1
        
        # Copy '–ü—É–±–ª–∏–∫–∞—Ü–∏–∏' sheet
        original_df = pd.read_excel(uploaded_file, sheet_name='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏')
        ws = wb['–ü—É–±–ª–∏–∫–∞—Ü–∏–∏']
        for r_idx, row in enumerate(dataframe_to_rows(original_df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)
        
        # Update '–ê–Ω–∞–ª–∏–∑' sheet
        ws = wb['–ê–Ω–∞–ª–∏–∑']
        row_idx = 4
        for _, row in df[df['Sentiment'] == 'Negative'].iterrows():
            ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])  # Column E
            ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])  # Column F
            ws.cell(row=row_idx, column=7, value="–†–∏—Å–∫ —É–±—ã—Ç–∫–∞")  # Column G
            
            # Translate reasoning if it exists
            if pd.notna(row['Reasoning']):
                translated_reasoning = translate_reasoning_to_russian(llm, row['Reasoning'])
                ws.cell(row=row_idx, column=8, value=translated_reasoning)  # Column H
            
            ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])  # Column I
            row_idx += 1
        
        # Update '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' sheet
        tech_df = df[['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 'Translated', 'Sentiment', 'Impact', 'Reasoning']]
        if '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' not in wb.sheetnames:
            wb.create_sheet('–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ')
        ws = wb['–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']
        for r_idx, row in enumerate(dataframe_to_rows(tech_df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)
    
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {str(e)}")
    
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    return output

def main():
    with st.sidebar:
        st.title("::: AI-–∞–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (v.3.35 ):::")
        st.subheader("–ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –°–ö–ê–ù-–ò–ù–¢–ï–†–§–ê–ö–° ")
        
        model_choice = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
            ["Groq (llama-3.1-70b)", "ChatGPT-4-mini", "Qwen-Max"],
            key="model_selector"
        )
        
        translation_method = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–≤–æ–¥–∞:",
            ["googletrans", "llm"],
            key="translation_selector",
            help="googletrans - –±—ã—Å—Ç—Ä–µ–µ, llm - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ"
        )
        
        st.markdown(
        """
        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:  
        - –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π **BERT**,<br/>
	    - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (**LLM**),<br/>
	    - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–∏ –ø–æ–º–æ—â–∏	—Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ **LangChain**.<br>
        """,
        unsafe_allow_html=True)

        with st.expander("‚ÑπÔ∏è –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"):
            st.markdown("""
            1. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            2. –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥ –ø–µ—Ä–µ–≤–æ–¥–∞
            3. –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
            4. –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
            5. –°–∫–∞—á–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel
            """, unsafe_allow_html=True)

   
        st.markdown(
        """
        <style>
        .signature {
            position: fixed;
            right: 12px;
            up: 12px;
            font-size: 14px;
            color: #FF0000;
            opacity: 0.9;
            z-index: 999;
        }
        </style>
        <div class="signature">denis.pokrovsky.npff</div>
        """,
        unsafe_allow_html=True
        )

    st.title("–ê–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
    
    # Single file uploader with unique key
    uploaded_file = st.sidebar.file_uploader("–í—ã–±–∏—Ä–∞–π—Ç–µ Excel-—Ñ–∞–π–ª", type="xlsx", key="unique_file_uploader")
    
    if uploaded_file is not None and st.session_state.processed_df is None:
        start_time = time.time()
        
        # Initialize LLM with selected model
        llm = init_langchain_llm(model_choice)

        # Process file with selected translation method
        st.session_state.processed_df = process_file(
            uploaded_file, 
            model_choice,
            translation_method
        )

        st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö")
        preview_df = st.session_state.processed_df[['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'Sentiment', 'Impact']].head()
        st.dataframe(preview_df)
        
        # Add preview of Monitoring results
        st.subheader("–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ–±—ã—Ç–∏–π –∏ —Ä–∏—Å–∫-—Ñ–∞–∫—Ç–æ—Ä–æ–≤ —ç–º–∏—Ç–µ–Ω—Ç–æ–≤")
        monitoring_df = st.session_state.processed_df[
            (st.session_state.processed_df['Event_Type'] != '–ù–µ—Ç') & 
            (st.session_state.processed_df['Event_Type'].notna())
        ][['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'Event_Type', 'Event_Summary']].head()
        
        if len(monitoring_df) > 0:
            st.dataframe(monitoring_df)
        else:
            st.info("–ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞")


        analysis_df = create_analysis_data(st.session_state.processed_df)
        st.subheader("–ê–Ω–∞–ª–∏–∑")
        st.dataframe(analysis_df)
        
       
        output = create_output_file(st.session_state.processed_df, uploaded_file, llm)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        formatted_time = format_elapsed_time(elapsed_time)
        st.success(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {formatted_time}.")

        st.download_button(
            label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞",
            data=output,
            file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

if __name__ == "__main__":
    main()