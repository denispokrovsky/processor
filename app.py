import streamlit as st
import pandas as pd
import time
import matplotlib.pyplot as plt
from openpyxl.utils.dataframe import dataframe_to_rows
import io
from rapidfuzz import fuzz
import os
from openpyxl import load_workbook
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from io import StringIO, BytesIO
import sys
import contextlib
from langchain_openai import ChatOpenAI  # Updated import
import pdfkit
from jinja2 import Template
import time
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Optional
import torch
from transformers import (
    pipeline,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    AutoModelForCausalLM  # 4 Qwen
)

from threading import Event
import threading
from queue import Queue

from deep_translator import GoogleTranslator
from googletrans import Translator as LegacyTranslator
import plotly.graph_objects as go
from datetime import datetime
import plotly.express as px


class ProcessControl:
    def __init__(self):
        self.pause_event = Event()
        self.stop_event = Event()
        self.pause_event.set()  # Start in non-paused state
        
    def pause(self):
        self.pause_event.clear()
        
    def resume(self):
        self.pause_event.set()
        
    def stop(self):
        self.stop_event.set()
        self.pause_event.set()  # Ensure not stuck in pause
        
    def reset(self):
        self.stop_event.clear()
        self.pause_event.set()
        
    def is_paused(self):
        return not self.pause_event.is_set()
        
    def is_stopped(self):
        return self.stop_event.is_set()
        
    def wait_if_paused(self):
        self.pause_event.wait()


class FallbackLLMSystem:
    def __init__(self):
        """Initialize fallback models for event detection and reasoning"""
        try:
            # Initialize MT5 model (multilingual T5)
            self.model_name = "google/mt5-small"
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            
            # Set device
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = self.model.to(self.device)
            
            st.success(f"–ø–æ–∫–∞ –≤—Å–µ –≤ –ø–æ—Ä—è–¥–∫–µ: –∑–∞–ø—É—â–µ–Ω–∞ MT5 model –Ω–∞ = {self.device} =")
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏ MT5: {str(e)}")
            raise

    def invoke(self, prompt_args):
        """Make the class compatible with LangChain by implementing invoke"""
        try:
            if isinstance(prompt_args, dict):
                # Extract the prompt template result
                template_result = prompt_args.get('template_result', '')
                if not template_result:
                    # Try to construct from entity and news if available
                    entity = prompt_args.get('entity', '')
                    news = prompt_args.get('news', '')
                    template_result = f"Analyze news about {entity}: {news}"
            else:
                template_result = str(prompt_args)

            # Process with MT5
            inputs = self.tokenizer(
                template_result,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            outputs = self.model.generate(
                **inputs,
                max_length=200,
                num_return_sequences=1,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id
            )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Return in a format compatible with LangChain
            return type('Response', (), {'content': response})()
            
        except Exception as e:
            st.warning(f"MT5 generation error: {str(e)}")
            # Return a default response on error
            return type('Response', (), {
                'content': 'Impact: –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç\nReasoning: –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞'
            })()

    def __or__(self, other):
        """Implement the | operator for chain compatibility"""
        if callable(other):
            return lambda x: other(self(x))
        return NotImplemented

    def __rrshift__(self, other):
        """Implement the >> operator for chain compatibility"""
        return self.__or__(other)

    def __call__(self, prompt_args):
        """Make the class callable for chain compatibility"""
        return self.invoke(prompt_args)

    def detect_events(self, text: str, entity: str) -> tuple[str, str]:
        """
        Detect events using MT5 with improved error handling and response parsing
        
        Args:
            text (str): The news text to analyze
            entity (str): The company/entity name
            
        Returns:
            tuple[str, str]: (event_type, summary)
        """
        # Initialize default return values
        event_type = "–ù–µ—Ç"
        summary = ""
        
        # Input validation
        if not text or not entity or not isinstance(text, str) or not isinstance(entity, str):
            return event_type, "Invalid input"
            
        try:
            # Clean and prepare input text
            text = text.strip()
            entity = entity.strip()
            
            # Construct prompt with better formatting
            prompt = f"""<s>Analyze the following news about {entity}:

    Text: {text}

    Task: Identify the main event type and provide a brief summary.

    Event types:
    1. –û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å - Events related to financial reports, earnings, revenue, EBITDA
    2. –†–¶–ë - Events related to securities, bonds, stock market, defaults, restructuring
    3. –°—É–¥ - Events related to legal proceedings, lawsuits, arbitration
    4. –ù–µ—Ç - No significant events detected

    Required output format:
    –¢–∏–ø: [event type]
    –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: [1-2 sentence summary]</s>"""

            # Process with MT5
            try:
                inputs = self.tokenizer(
                    prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                ).to(self.device)
                
                outputs = self.model.generate(
                    **inputs,
                    max_length=300,  # Increased for better summaries
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3  # Prevent repetition
                )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
            except torch.cuda.OutOfMemoryError:
                st.warning("GPU memory exceeded, falling back to CPU")
                self.model = self.model.to('cpu')
                inputs = inputs.to('cpu')
                outputs = self.model.generate(
                    **inputs,
                    max_length=300,
                    num_return_sequences=1,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id
                )
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                self.model = self.model.to(self.device)  # Move back to GPU
                
            # Enhanced response parsing
            if "–¢–∏–ø:" in response and "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:" in response:
                try:
                    # Split and clean parts
                    parts = response.split("–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ:")
                    type_part = parts[0].split("–¢–∏–ø:")[1].strip()
                    
                    # Validate event type with fuzzy matching
                    valid_types = ["–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "–†–¶–ë", "–°—É–¥", "–ù–µ—Ç"]
                    
                    # Check for exact matches first
                    if type_part in valid_types:
                        event_type = type_part
                    else:
                        # Check keywords for each type
                        keywords = {
                            "–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å": ["–æ—Ç—á–µ—Ç", "–≤—ã—Ä—É—á–∫–∞", "–ø—Ä–∏–±—ã–ª—å", "ebitda", "—Ñ–∏–Ω–∞–Ω—Å"],
                            "–†–¶–ë": ["–æ–±–ª–∏–≥–∞—Ü–∏", "–∫—É–ø–æ–Ω", "–¥–µ—Ñ–æ–ª—Ç", "—Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü", "—Ü–µ–Ω–Ω—ã–µ –±—É–º–∞–≥–∏"],
                            "–°—É–¥": ["—Å—É–¥", "–∏—Å–∫", "–∞—Ä–±–∏—Ç—Ä–∞–∂", "—Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤"]
                        }
                        
                        # Look for keywords in both type and summary
                        full_text = response.lower()
                        for event_category, category_keywords in keywords.items():
                            if any(keyword in full_text for keyword in category_keywords):
                                event_type = event_category
                                break
                    
                    # Extract and clean summary
                    if len(parts) > 1:
                        summary = parts[1].strip()
                        # Ensure summary isn't too long
                        if len(summary) > 200:
                            summary = summary[:197] + "..."
                        
                        # Add entity reference if missing
                        if entity.lower() not in summary.lower():
                            summary = f"–ö–æ–º–ø–∞–Ω–∏—è {entity}: {summary}"
                    
                except IndexError:
                    st.warning("Error parsing model response format")
                    return "–ù–µ—Ç", "Error parsing response"
                    
            # Additional validation
            if not summary or len(summary) < 5:
                keywords = {
                    "–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏",
                    "–†–¶–ë": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–µ–Ω–Ω—ã—Ö –±—É–º–∞–≥–∞—Ö",
                    "–°—É–¥": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—É–¥–µ–±–Ω–æ–º —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–µ",
                    "–ù–µ—Ç": "–ó–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ"
                }
                summary = f"{keywords.get(event_type, '–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑')} ({entity})"
                
            return event_type, summary
                
        except Exception as e:
            st.warning(f"Event detection error: {str(e)}")
            # Try to provide more specific error information
            if "CUDA" in str(e):
                return "–ù–µ—Ç", "GPU error - falling back to CPU needed"
            elif "tokenizer" in str(e):
                return "–ù–µ—Ç", "Text processing error"
            elif "model" in str(e):
                return "–ù–µ—Ç", "Model inference error"
            else:
                return "–ù–µ—Ç", "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"
        

def ensure_groq_llm():
    """Initialize Groq LLM for impact estimation"""
    try:
        if 'groq_key' not in st.secrets:
            st.error("Groq API key not found in secrets. Please add it with the key 'groq_key'.")
            return None
            
        return ChatOpenAI(
            base_url="https://api.groq.com/openai/v1",
            model="llama-3.1-70b-versatile",
            openai_api_key=st.secrets['groq_key'],
            temperature=0.0
        )
    except Exception as e:
        st.error(f"Error initializing Groq LLM: {str(e)}")
        return None

def estimate_impact(llm, news_text, entity):
    """
    Estimate impact using Groq LLM regardless of the main model choice.
    Falls back to the provided LLM if Groq initialization fails.
    """
    # Initialize default return values
    impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
    reasoning = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ"
    
    try:
        # Always try to use Groq first
        groq_llm = ensure_groq_llm()
        working_llm = groq_llm if groq_llm is not None else llm
        
        template = """
        You are a financial analyst. Analyze this news piece about {entity} and assess its potential impact.
        
        News: {news}
        
        Classify the impact into one of these categories:
        1. "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Significant loss risk)
        2. "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Moderate loss risk)
        3. "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" (Minor loss risk)
        4. "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏" (Potential profit)
        5. "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç" (Uncertain effect)
        
        Provide a brief, fact-based reasoning for your assessment.
        
        Format your response exactly as:
        Impact: [category]
        Reasoning: [explanation in 2-3 sentences]
        """
        
        prompt = PromptTemplate(template=template, input_variables=["entity", "news"])
        chain = prompt | working_llm
        response = chain.invoke({"entity": entity, "news": news_text})
        
        # Extract content from response
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        if "Impact:" in response_text and "Reasoning:" in response_text:
            impact_part, reasoning_part = response_text.split("Reasoning:")
            impact_temp = impact_part.split("Impact:")[1].strip()
            
            # Validate impact category
            valid_impacts = [
                "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏",
                "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
            ]
            if impact_temp in valid_impacts:
                impact = impact_temp
            reasoning = reasoning_part.strip()
            
    except Exception as e:
        st.warning(f"Error in impact estimation: {str(e)}")
    
    return impact, reasoning
   
class QwenSystem:
    def __init__(self):
        """Initialize Qwen 2.5 Coder model"""
        try:
            self.model_name = "Qwen/Qwen2.5-Coder-32B-Instruct"
            
            # Initialize model with auto settings
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            st.success(f"–∑–∞–ø—É—Å—Ç–∏–ª Qwen2.5 model")
            
        except Exception as e:
            st.error(f"–æ—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Qwen2.5: {str(e)}")
            raise

    def invoke(self, messages):
        """Process messages using Qwen's chat template"""
        try:
            # Prepare messages with system prompt
            chat_messages = [
                {"role": "system", "content": "You are wise financial analyst. You are a helpful assistant."}
            ]
            chat_messages.extend(messages)
            
            # Apply chat template
            text = self.tokenizer.apply_chat_template(
                chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Prepare model inputs
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # Generate response
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
            # Extract new tokens
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            # Decode response
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # Return in ChatOpenAI-compatible format
            return type('Response', (), {'content': response})()
            
        except Exception as e:
            st.warning(f"Qwen generation error: {str(e)}")
            raise


class ProcessingUI:
    def __init__(self):
        if 'control' not in st.session_state:
            st.session_state.control = ProcessControl()
            
        # Initialize processing stats in session state if not exists
        if 'processing_stats' not in st.session_state:
            st.session_state.processing_stats = {
                'start_time': time.time(),
                'entities': {},
                'events_timeline': [],
                'negative_alerts': [],
                'processing_speed': []
            }
        
        # Create main layout
        self.setup_layout()
        
    def setup_layout(self):
        """Setup the main UI layout with tabs and sections"""
        # Control Panel
        with st.container():
            col1, col2, col3 = st.columns([2,2,1])
            with col1:
                if st.button(
                    "‚è∏Ô∏è –ü–∞—É–∑–∞" if not st.session_state.control.is_paused() else "‚ñ∂Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å",
                    use_container_width=True
                ):
                    if st.session_state.control.is_paused():
                        st.session_state.control.resume()
                    else:
                        st.session_state.control.pause()
            with col2:
                if st.button("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å", use_container_width=True):
                    st.session_state.control.stop()
            with col3:
                self.timer_display = st.empty()
        
        # Progress Bar with custom styling
        st.markdown("""
            <style>
            .stProgress > div > div > div > div {
                background-image: linear-gradient(to right, #FF6B6B, #4ECDC4);
            }
            </style>""", 
            unsafe_allow_html=True
        )
        self.progress_bar = st.progress(0)
        self.status = st.empty()

        # Create tabs for different views
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏", 
            "üè¢ –ü–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º", 
            "‚ö†Ô∏è –í–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è", 
            "üìà –ê–Ω–∞–ª–∏—Ç–∏–∫–∞"
        ])
        
        with tab1:
            self.setup_main_metrics_tab()
            
        with tab2:
            self.setup_entity_tab()
            
        with tab3:
            self.setup_events_tab()
            
        with tab4:
            self.setup_analytics_tab()
                  
    def setup_entity_tab(self):
        """Setup the entity-wise analysis display"""
        # Entity filter
        self.entity_filter = st.multiselect(
            "–§–∏–ª—å—Ç—Ä –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º:",
            options=[],  # Will be populated as entities are processed
            default=None
        )
        
        # Entity metrics
        self.entity_cols = st.columns([2,1,1,1])
        self.entity_chart = st.empty()
        self.entity_table = st.empty()
        
    def setup_events_tab(self):
        """Setup the events timeline display"""
        # Event type filter - store in session state
        if 'event_filter' not in st.session_state:
            st.session_state.event_filter = []
            
        st.session_state.event_filter = st.multiselect(
            "–¢–∏–ø —Å–æ–±—ã—Ç–∏—è:",
            options=["–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "–†–¶–ë", "–°—É–¥"],
            default=None,
            key="event_filter_key"
        )
        
        self.timeline_container = st.container()
    
    def _update_events_view(self, row, event_type):
        """Update events timeline"""
        if event_type != '–ù–µ—Ç':
            event_html = f"""
                <div class='timeline-item' style='
                    border-left: 4px solid #2196F3;
                    margin: 10px 0;
                    padding: 10px;
                    background: #f5f5f5;
                    border-radius: 4px;
                '>
                    <h4 style='color: #2196F3; margin: 0;'>{event_type}</h4>
                    <p><strong>{row['–û–±—ä–µ–∫—Ç']}</strong></p>
                    <p>{row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}</p>
                    <p style='font-size: 0.9em;'>{row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']}</p>
                    <small style='color: #666;'>{datetime.now().strftime('%H:%M:%S')}</small>
                </div>
            """
            with self.timeline_container:
                st.markdown(event_html, unsafe_allow_html=True)

    def setup_analytics_tab(self):
        """Setup the analytics display"""
        # Create containers for analytics
        self.speed_container = st.container()
        with self.speed_container:
            st.subheader("–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            self.speed_chart = st.empty()
            
        self.sentiment_container = st.container()
        with self.sentiment_container:
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")
            self.sentiment_chart = st.empty()
            
        self.correlation_container = st.container()
        with self.correlation_container:
            st.subheader("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏")
            self.correlation_chart = st.empty()
        
    def update_stats(self, row, sentiment, event_type, processing_speed):
        """Update all statistics and displays"""
        # Update session state stats
        stats = st.session_state.processing_stats
        entity = row['–û–±—ä–µ–∫—Ç']
        
        # Update entity stats
        if entity not in stats['entities']:
            stats['entities'][entity] = {
                'total': 0,
                'negative': 0,
                'events': 0,
                'timeline': []
            }
        
        stats['entities'][entity]['total'] += 1
        if sentiment == 'Negative':
            stats['entities'][entity]['negative'] += 1
        if event_type != '–ù–µ—Ç':
            stats['entities'][entity]['events'] += 1
            
        # Update processing speed
        stats['processing_speed'].append(processing_speed)
        
        # Update UI components
        self._update_main_metrics(row, sentiment, event_type, processing_speed)
        self._update_entity_view()
        self._update_events_view(row, event_type)
        self._update_analytics()
        
    def _update_main_metrics(self, row, sentiment, event_type, speed):
        """Update main metrics tab"""
        total = sum(e['total'] for e in st.session_state.processing_stats['entities'].values())
        total_negative = sum(e['negative'] for e in st.session_state.processing_stats['entities'].values())
        total_events = sum(e['events'] for e in st.session_state.processing_stats['entities'].values())
        
        # Update metrics
        self.total_processed.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ", total)
        self.negative_count.metric("–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö", total_negative)
        self.events_count.metric("–°–æ–±—ã—Ç–∏–π", total_events)
        self.speed_metric.metric("–°–∫–æ—Ä–æ—Å—Ç—å", f"{speed:.1f} —Å–æ–æ–±—â/—Å–µ–∫")
        
        # Update recent items
        self._update_recent_items(row, sentiment, event_type)
        
    def _update_recent_items(self, row, sentiment, event_type):
        """Update recent items display using Streamlit native components"""
        if 'recent_items' not in st.session_state:
            st.session_state.recent_items = []
            
        # Add new item to the list
        new_item = {
            'entity': row['–û–±—ä–µ–∫—Ç'],
            'headline': row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'],
            'sentiment': sentiment,
            'event_type': event_type,
            'time': datetime.now().strftime('%H:%M:%S')
        }
        
        # Update the list in session state
        if not any(
            item['entity'] == new_item['entity'] and 
            item['headline'] == new_item['headline'] 
            for item in st.session_state.recent_items
        ):
            st.session_state.recent_items.insert(0, new_item)
            st.session_state.recent_items = st.session_state.recent_items[:10]  # Keep last 10 items

        # Prepare markdown for all items
        all_items_markdown = ""
        
        for item in st.session_state.recent_items:
            if item['sentiment'] in ['Positive', 'Negative']:
                sentiment_color = "üî¥" if item['sentiment'] == 'Negative' else "üü¢"
                event_icon = "üìÖ" if item['event_type'] != '–ù–µ—Ç' else ""
                
                event_text = f" | –°–æ–±—ã—Ç–∏–µ: {item['event_type']}" if item['event_type'] != '–ù–µ—Ç' else ""
                
                all_items_markdown += f"""
                {sentiment_color} **{item['entity']}**  {event_icon}

                {item['headline']}

                *{item['sentiment']}*{event_text} | {item['time']}

                ---
                """
        
        # Update container with all items at once
        if all_items_markdown:
            self.recent_items_container.markdown(all_items_markdown)

    def setup_main_metrics_tab(self):
        """Setup the main metrics display with updated styling"""
        # Create metrics containers
        metrics_cols = st.columns(4)
        self.total_processed = metrics_cols[0].empty()
        self.negative_count = metrics_cols[1].empty()
        self.events_count = metrics_cols[2].empty()
        self.speed_metric = metrics_cols[3].empty()
        
        # Create container for recent items
        st.markdown("### –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ/–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ")
        self.recent_items_container = st.empty()


    def _update_entity_view(self):
        """Update entity tab visualizations"""
        stats = st.session_state.processing_stats['entities']
        if not stats:
            return
            
        # Get filtered entities
        filtered_entities = self.entity_filter or stats.keys()
        
        # Create entity comparison chart using Plotly
        df_entities = pd.DataFrame.from_dict(stats, orient='index')
        df_entities = df_entities.loc[filtered_entities]  # Apply filter
        
        fig = go.Figure(data=[
            go.Bar(
                name='–í—Å–µ–≥–æ',
                x=df_entities.index,
                y=df_entities['total'],
                marker_color='#E0E0E0'  # Light gray
            ),
            go.Bar(
                name='–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ',
                x=df_entities.index,
                y=df_entities['negative'],
                marker_color='#FF6B6B'  # Red
            ),
            go.Bar(
                name='–°–æ–±—ã—Ç–∏—è',
                x=df_entities.index,
                y=df_entities['events'],
                marker_color='#2196F3'  # Blue
            )
        ])
        
        fig.update_layout(
            barmode='group',
            title='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º',
            xaxis_title='–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',
            yaxis_title='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
            showlegend=True
        )
        
        self.entity_chart.plotly_chart(fig, use_container_width=True)
                
    def _update_analytics(self):
        """Update analytics tab visualizations"""
        stats = st.session_state.processing_stats
        
        # Processing speed chart - showing last 20 measurements
        speeds = stats['processing_speed'][-20:]
        if speeds:
            fig_speed = go.Figure(data=go.Scatter(
                y=speeds,
                mode='lines+markers',
                name='–°–∫–æ—Ä–æ—Å—Ç—å',
                line=dict(color='#4CAF50')
            ))
            fig_speed.update_layout(
                title='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏',
                yaxis_title='–°–æ–æ–±—â–µ–Ω–∏–π –≤ —Å–µ–∫—É–Ω–¥—É',
                showlegend=True
            )
            self.speed_chart.plotly_chart(fig_speed, use_container_width=True)
            
        # Sentiment distribution pie chart
        if stats['entities']:
            total_negative = sum(e['negative'] for e in stats['entities'].values())
            total_positive = sum(e['events'] for e in stats['entities'].values())
            total_neutral = sum(e['total'] for e in stats['entities'].values()) - total_negative - total_positive
            
            fig_sentiment = go.Figure(data=[go.Pie(
                labels=['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ', '–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ', '–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ'],
                values=[total_negative, total_positive, total_neutral],
                marker_colors=['#FF6B6B', '#4ECDC4', '#95A5A6']
            )])
            self.sentiment_chart.plotly_chart(fig_sentiment, use_container_width=True)
        
    def update_progress(self, current, total):
        """Update progress bar, elapsed time and estimated time remaining"""
        progress = current / total
        self.progress_bar.progress(progress)
        self.status.text(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {current} –∏–∑ {total} —Å–æ–æ–±—â–µ–Ω–∏–π...")
        
        # Calculate times
        current_time = time.time()
        elapsed = current_time - st.session_state.processing_stats['start_time']
        
        # Calculate processing speed and estimated time remaining
        if current > 0:
            speed = current / elapsed  # items per second
            remaining_items = total - current
            estimated_remaining = remaining_items / speed if speed > 0 else 0
            
            time_display = (
                f"‚è±Ô∏è –ü—Ä–æ—à–ª–æ: {format_elapsed_time(elapsed)} | "
                f"–û—Å—Ç–∞–ª–æ—Å—å: {format_elapsed_time(estimated_remaining)}"
            )
        else:
            time_display = f"‚è±Ô∏è –ü—Ä–æ—à–ª–æ: {format_elapsed_time(elapsed)}"
            
        self.timer_display.markdown(time_display)


class EventDetectionSystem:
    def __init__(self):
        try:
            # Initialize models with specific labels
            self.finbert = pipeline(
                "text-classification", 
                model="ProsusAI/finbert",
                return_all_scores=True
            )
            self.business_classifier = pipeline(
                "text-classification", 
                model="yiyanghkust/finbert-tone",
                return_all_scores=True
            )
            st.success("–ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –ø–æ–∫–∞ —Ö–æ—Ä–æ—à–æ: BERT-–º–æ–¥–µ–ª–∏ –∑–∞–ø—É—â–µ–Ω—ã –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ BERT: {str(e)}")
            raise

    def detect_event_type(self, text, entity):
        event_type = "–ù–µ—Ç"
        summary = ""
        
        try:
            # Ensure text is properly formatted
            text = str(text).strip()
            if not text:
                return "–ù–µ—Ç", "Empty text"

            # Get predictions
            finbert_scores = self.finbert(
                text,
                truncation=True,
                max_length=512
            )
            business_scores = self.business_classifier(
                text,
                truncation=True,
                max_length=512
            )
            
            # Get highest scoring predictions
            finbert_pred = max(finbert_scores[0], key=lambda x: x['score'])
            business_pred = max(business_scores[0], key=lambda x: x['score'])
            
            # Map to event types with confidence threshold
            confidence_threshold = 0.6
            max_confidence = max(finbert_pred['score'], business_pred['score'])
            
            if max_confidence >= confidence_threshold:
                if any(term in text.lower() for term in ['–æ—Ç—á–µ—Ç', '–≤—ã—Ä—É—á–∫–∞', '–ø—Ä–∏–±—ã–ª—å', 'ebitda']):
                    event_type = "–û—Ç—á–µ—Ç–Ω–æ—Å—Ç—å"
                    summary = f"–§–∏–Ω–∞–Ω—Å–æ–≤–∞—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å (confidence: {max_confidence:.2f})"
                elif any(term in text.lower() for term in ['–æ–±–ª–∏–≥–∞—Ü–∏', '–∫—É–ø–æ–Ω', '–¥–µ—Ñ–æ–ª—Ç', '—Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü']):
                    event_type = "–†–¶–ë"
                    summary = f"–°–æ–±—ã—Ç–∏–µ –†–¶–ë (confidence: {max_confidence:.2f})"
                elif any(term in text.lower() for term in ['—Å—É–¥', '–∏—Å–∫', '–∞—Ä–±–∏—Ç—Ä–∞–∂']):
                    event_type = "–°—É–¥"
                    summary = f"–°—É–¥–µ–±–Ω–æ–µ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–æ (confidence: {max_confidence:.2f})"
            
            if event_type != "–ù–µ—Ç":
                summary += f"\n–ö–æ–º–ø–∞–Ω–∏—è: {entity}"
            
            return event_type, summary
            
        except Exception as e:
            st.warning(f"Event detection error: {str(e)}")
            return "–ù–µ—Ç", "Error in event detection"

class TranslationSystem:
    def __init__(self):
        """Initialize translation system using Helsinki NLP model with fallback options"""
        try:
            self.translator = pipeline("translation", model="Helsinki-NLP/opus-mt-ru-en")
            # Initialize fallback translator
            self.fallback_translator = GoogleTranslator(source='ru', target='en')
            self.legacy_translator = LegacyTranslator()
            st.success("–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –≤—Å–µ —Ö–æ—Ä–æ—à–æ: –∑–∞–ø—É—Å—Ç–∏–ª —Å–∏—Å—Ç–µ–º—É –ø–µ—Ä–µ–≤–æ–¥–∞")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {str(e)}")
            raise

    def _split_into_chunks(self, text: str, max_length: int = 450) -> list:
        """Split text into chunks while preserving word boundaries"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0

        for word in words:
            word_length = len(word)
            if current_length + word_length + 1 <= max_length:
                current_chunk.append(word)
                current_length += word_length + 1
            else:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = word_length

        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _translate_chunk_with_retries(self, chunk: str, max_retries: int = 3) -> str:
        """Attempt translation with multiple fallback options"""
        if not chunk or not chunk.strip():
            return ""

        for attempt in range(max_retries):
            try:
                # First try Helsinki NLP
                result = self.translator(chunk, max_length=512)
                if result and isinstance(result, list) and len(result) > 0:
                    translated = result[0].get('translation_text')
                    if translated and isinstance(translated, str):
                        return translated

                # First fallback: Google Translator
                translated = self.fallback_translator.translate(chunk)
                if translated and isinstance(translated, str):
                    return translated

                # Second fallback: Legacy Google Translator
                translated = self.legacy_translator.translate(chunk, src='ru', dest='en').text
                if translated and isinstance(translated, str):
                    return translated

            except Exception as e:
                if attempt == max_retries - 1:
                    st.warning(f"–ü–æ–ø—Ä–æ–±–æ–≤–∞–ª –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ {max_retries} —Ä–∞–∑, –Ω–µ –ø—Ä–µ—É—Å–ø–µ–ª: {str(e)}")
                time.sleep(1 * (attempt + 1))  # Exponential backoff

        return chunk  # Return original text if all translation attempts fail

    def translate_text(self, text: str) -> str:
        """Translate text with robust error handling and validation"""
        # Input validation
        if pd.isna(text) or not isinstance(text, str):
            return str(text) if pd.notna(text) else ""

        text = str(text).strip()
        if not text:
            return ""

        try:
            # Split into manageable chunks
            chunks = self._split_into_chunks(text)
            translated_chunks = []

            # Process each chunk with validation
            for chunk in chunks:
                if not chunk.strip():
                    continue

                translated_chunk = self._translate_chunk_with_retries(chunk)
                if translated_chunk:  # Only add non-empty translations
                    translated_chunks.append(translated_chunk)
                time.sleep(0.1)  # Rate limiting

            # Final validation of results
            if not translated_chunks:
                return text  # Return original if no translations succeeded

            result = ' '.join(translated_chunks)
            return result if result.strip() else text

        except Exception as e:
            st.warning(f"Translation error: {str(e)}")
            return text  # Return original text on error



def process_file(uploaded_file, model_choice, translation_method=None):
    df = None
    processed_rows_df = pd.DataFrame()
    last_update_time = time.time()

    try:
        # Initialize UI and control systems
        ui = ProcessingUI()
        translator = TranslationSystem()
        event_detector = EventDetectionSystem()
        
        # Load and prepare data
        df = pd.read_excel(uploaded_file, sheet_name='–ü—É–±–ª–∏–∫–∞—Ü–∏–∏')
        llm = init_langchain_llm(model_choice)
        
        # Initialize Groq for impact estimation
        groq_llm = ensure_groq_llm()
        if groq_llm is None:
            st.warning("Failed to initialize Groq LLM for impact estimation. Using fallback model.")
        
        # Initialize all required columns at the start
        required_columns = {
            '–û–±—ä–µ–∫—Ç': '',
            '–ó–∞–≥–æ–ª–æ–≤–æ–∫': '',
            '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞': '',
            'Translated': '',
            'Sentiment': 'Neutral',
            'Impact': '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç',
            'Reasoning': '–ù–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ',
            'Event_Type': '–ù–µ—Ç',
            'Event_Summary': ''
        }
        
        # Ensure all required columns exist in DataFrame
        for col, default_value in required_columns.items():
            if col not in df.columns:
                df[col] = default_value
        
        # Create processed_rows_df with all columns from original df and required columns
        all_columns = list(set(list(df.columns) + list(required_columns.keys())))
        processed_rows_df = pd.DataFrame(columns=all_columns)
        
        # Process rows
        total_rows = len(df)
        processed_rows = 0
        
        for idx, row in df.iterrows():
            if st.session_state.control.is_stopped():
                st.warning("–û–±—Ä–∞–±–æ—Ç–∫—É –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏")
                if not processed_rows_df.empty:
                    try:
                        # Create the output files for each sheet
                        monitoring_df = processed_rows_df[processed_rows_df['Event_Type'] != '–ù–µ—Ç'].copy()
                        svodka_df = processed_rows_df.groupby('–û–±—ä–µ–∫—Ç').agg({
                            '–û–±—ä–µ–∫—Ç': 'first',
                            'Sentiment': lambda x: sum(x == 'Negative'),
                            'Event_Type': lambda x: sum(x != '–ù–µ—Ç')
                        }).reset_index()
                        
                        # Prepare final DataFrame for file creation
                        result_df = pd.DataFrame()
                        result_df['–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥'] = monitoring_df.to_dict('records')
                        result_df['–°–≤–æ–¥–∫–∞'] = svodka_df.to_dict('records')
                        result_df['–ü—É–±–ª–∏–∫–∞—Ü–∏–∏'] = processed_rows_df.to_dict('records')
                        
                        output = create_output_file(result_df, uploaded_file, llm)
                        if output is not None:
                            st.download_button(
                                label=f"üìä –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç ({processed_rows} –∏–∑ {total_rows} —Å—Ç—Ä–æ–∫)",
                                data=output,
                                file_name="partial_analysis.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                key="partial_download"
                            )
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
                        
                return processed_rows_df
                
            st.session_state.control.wait_if_paused()
            if st.session_state.control.is_paused():
                continue
                
            try:
                # Copy original row data
                new_row = row.copy()
                
                # Translation
                translated_text = translator.translate_text(row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
                new_row['Translated'] = translated_text
                
                # Sentiment analysis
                sentiment = analyze_sentiment(translated_text)
                new_row['Sentiment'] = sentiment
                
                # Event detection
                event_type, event_summary = event_detector.detect_event_type(
                    row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'],
                    row['–û–±—ä–µ–∫—Ç']
                )
                new_row['Event_Type'] = event_type
                new_row['Event_Summary'] = event_summary
                
                # Handle negative sentiment
                if sentiment == "Negative":
                    try:
                        if translated_text and len(translated_text.strip()) > 0:
                            impact, reasoning = estimate_impact(
                                groq_llm if groq_llm is not None else llm,
                                translated_text,
                                row['–û–±—ä–µ–∫—Ç']
                            )
                            new_row['Impact'] = impact
                            new_row['Reasoning'] = reasoning
                    except Exception as e:
                        new_row['Impact'] = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
                        new_row['Reasoning'] = "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞"
                
                # Add processed row to DataFrame
                processed_rows_df = pd.concat([processed_rows_df, pd.DataFrame([new_row])], ignore_index=True)
                
                # Update progress
                processed_rows += 1
                ui.update_progress(processed_rows, total_rows)
                
            except Exception as e:
                st.warning(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä—è–¥–∞ {idx + 1}: {str(e)}")
                continue
                
        return processed_rows_df
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
        return None


    

def create_download_section(excel_data, pdf_data):
    st.markdown("""
        <div class="download-container">
            <div class="download-header">üì• –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è:</div>
        </div>
    """, unsafe_allow_html=True)

    col1, col2 = st.columns(2)
    
    with col1:
        if excel_data is not None:
            st.download_button(
                label="üìä –°–∫–∞—á–∞—Ç—å Excel –æ—Ç—á–µ—Ç",
                data=excel_data,
                file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key="excel_download"
            )
        else:
            st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Excel —Ñ–∞–π–ª–∞")
    



def display_sentiment_results(row, sentiment, impact=None, reasoning=None):
    if sentiment == "Negative":
        st.markdown(f"""
            <div style='color: red; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            {"–≠—Ñ—Ñ–µ–∫—Ç: " + impact + "<br>" if impact else ""}
            {"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: " + reasoning + "<br>" if reasoning else ""}
            </div>
            """, unsafe_allow_html=True)
    elif sentiment == "Positive":
        st.markdown(f"""
            <div style='color: green; font-weight: bold;'>
            –û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}<br>
            –ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}<br>
            –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}<br>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.write(f"–û–±—ä–µ–∫—Ç: {row['–û–±—ä–µ–∫—Ç']}")
        st.write(f"–ù–æ–≤–æ—Å—Ç—å: {row['–ó–∞–≥–æ–ª–æ–≤–æ–∫']}")
        st.write(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment}")
    
    st.write("---")




    
# Initialize sentiment analyzers
finbert = pipeline("sentiment-analysis", model="ProsusAI/finbert")
roberta = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment")
finbert_tone = pipeline("sentiment-analysis", model="yiyanghkust/finbert-tone")


def get_mapped_sentiment(result):
    label = result['label'].lower()
    if label in ["positive", "label_2", "pos", "pos_label"]:
        return "Positive"
    elif label in ["negative", "label_0", "neg", "neg_label"]:
        return "Negative"
    return "Neutral"



def analyze_sentiment(text):
    try:
        finbert_result = get_mapped_sentiment(
            finbert(text, truncation=True, max_length=512)[0]
        )
        roberta_result = get_mapped_sentiment(
            roberta(text, truncation=True, max_length=512)[0]
        )
        finbert_tone_result = get_mapped_sentiment(
            finbert_tone(text, truncation=True, max_length=512)[0]
        )
        
        # Count occurrences of each sentiment
        sentiments = [finbert_result, roberta_result, finbert_tone_result]
        sentiment_counts = {s: sentiments.count(s) for s in set(sentiments)}
        
        # Return sentiment if at least two models agree
        for sentiment, count in sentiment_counts.items():
            if count >= 2:
                return sentiment
                
        # Default to Neutral if no agreement
        return "Neutral"
        
    except Exception as e:
        st.warning(f"Sentiment analysis error: {str(e)}")
        return "Neutral"


def fuzzy_deduplicate(df, column, threshold=50):
    seen_texts = []
    indices_to_keep = []
    for i, text in enumerate(df[column]):
        if pd.isna(text):
            indices_to_keep.append(i)
            continue
        text = str(text)
        if not seen_texts or all(fuzz.ratio(text, seen) < threshold for seen in seen_texts):
            seen_texts.append(text)
            indices_to_keep.append(i)
    return df.iloc[indices_to_keep]


def init_langchain_llm(model_choice):
    try:
        if model_choice == "Qwen2.5-Coder":
            st.info("Loading Qwen2.5-Coder model. —Ç–æ–ª—å–∫–æ GPU!")
            return QwenSystem()
            
        elif model_choice == "Groq (llama-3.1-70b)":
            if 'groq_key' not in st.secrets:
                st.error("Groq API key not found in secrets. Please add it with the key 'groq_key'.")
                st.stop()
                
            return ChatOpenAI(
                base_url="https://api.groq.com/openai/v1",
                model="llama-3.1-70b-versatile",
                openai_api_key=st.secrets['groq_key'],
                temperature=0.0
            )
            
        elif model_choice == "ChatGPT-4-mini":
            if 'openai_key' not in st.secrets:
                st.error("OpenAI API key not found in secrets. Please add it with the key 'openai_key'.")
                st.stop()
                
            return ChatOpenAI(
                model="gpt-4",
                openai_api_key=st.secrets['openai_key'],
                temperature=0.0
            )
            
        elif model_choice == "Local-MT5":
            return FallbackLLMSystem()
            
    except Exception as e:
        st.error(f"Error initializing the LLM: {str(e)}")
        st.stop()


def estimate_impact(llm, news_text, entity):
    """
    Estimate impact using Groq LLM with improved error handling and validation.
    """
    try:
        # Input validation
        if not news_text or not entity:
            return "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç", "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
            
        # Clean up inputs
        news_text = str(news_text).strip()
        entity = str(entity).strip()
        
        # Always try to use Groq first
        working_llm = ensure_groq_llm() if 'groq_key' in st.secrets else llm
        
        template = """
        You are a financial analyst tasked with assessing the impact of news on a company.
        
        Company: {entity}
        News Text: {news}
        
        Based on the news content, strictly classify the potential impact into ONE of these categories:
        1. "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" - For severe negative events like bankruptcy, major legal issues, significant market loss
        2. "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" - For moderate negative events like minor legal issues, temporary setbacks
        3. "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤" - For minor negative events with limited impact
        4. "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏" - For positive events that could lead to profit or growth
        5. "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç" - Only if impact cannot be determined from the information
        
        FORMAT YOUR RESPONSE EXACTLY AS:
        Impact: [category name exactly as shown above]
        Reasoning: [2-3 concise sentences explaining your choice]
        """
        
        prompt = PromptTemplate(template=template, input_variables=["entity", "news"])
        chain = prompt | working_llm
        
        # Make the API call
        response = chain.invoke({
            "entity": entity,
            "news": news_text
        })
        
        # Parse response
        response_text = response.content if hasattr(response, 'content') else str(response)
        
        # Extract impact and reasoning
        impact = "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"  # Default
        reasoning = "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≤–ª–∏—è–Ω–∏–µ"  # Default
        
        if "Impact:" in response_text and "Reasoning:" in response_text:
            parts = response_text.split("Reasoning:")
            impact_part = parts[0].split("Impact:")[1].strip()
            reasoning = parts[1].strip()
            
            # Validate impact category with fuzzy matching
            valid_impacts = [
                "–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ —É–±—ã—Ç–∫–æ–≤",
                "–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏",
                "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç"
            ]
            
            # Use fuzzy matching
            best_match = None
            best_score = 0
            for valid_impact in valid_impacts:
                score = fuzz.ratio(impact_part.lower(), valid_impact.lower())
                if score > best_score and score > 80:  # 80% similarity threshold
                    best_score = score
                    best_match = valid_impact
            
            if best_match:
                impact = best_match
        
        return impact, reasoning
        
    except Exception as e:
        st.warning(f"Impact estimation error: {str(e)}")
        if 'rate limit' in str(e).lower():
            st.warning("Rate limit reached. Using fallback analysis.")
        return "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç", "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≤–ª–∏—è–Ω–∏—è"

def format_elapsed_time(seconds):
    hours, remainder = divmod(int(seconds), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    time_parts = []
    if hours > 0:
        time_parts.append(f"{hours} —á–∞—Å{'–æ–≤' if hours != 1 else ''}")
    if minutes > 0:
        time_parts.append(f"{minutes} –º–∏–Ω—É—Ç{'' if minutes == 1 else '—ã' if 2 <= minutes <= 4 else ''}")
    if seconds > 0 or not time_parts:
        time_parts.append(f"{seconds} —Å–µ–∫—É–Ω–¥{'–∞' if seconds == 1 else '—ã' if 2 <= seconds <= 4 else ''}")
    
    return " ".join(time_parts)

def generate_sentiment_visualization(df):
    negative_df = df[df['Sentiment'] == 'Negative']
    
    if negative_df.empty:
        st.warning("–ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ–±—ä–µ–∫—Ç–∞–º.")
        entity_counts = df['–û–±—ä–µ–∫—Ç'].value_counts()
    else:
        entity_counts = negative_df['–û–±—ä–µ–∫—Ç'].value_counts()
    
    if len(entity_counts) == 0:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.")
        return None
    
    fig, ax = plt.subplots(figsize=(12, max(6, len(entity_counts) * 0.5)))
    entity_counts.plot(kind='barh', ax=ax)
    ax.set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ –æ–±—ä–µ–∫—Ç–∞–º')
    ax.set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π')
    plt.tight_layout()
    return fig

def create_analysis_data(df):
    analysis_data = []
    for _, row in df.iterrows():
        if row['Sentiment'] == 'Negative':
            analysis_data.append([
                row['–û–±—ä–µ–∫—Ç'], 
                row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'], 
                '–†–ò–°–ö –£–ë–´–¢–ö–ê', 
                row['Impact'],
                row['Reasoning'],
                row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞']
            ])
    return pd.DataFrame(analysis_data, columns=[
        '–û–±—ä–µ–∫—Ç', 
        '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 
        '–ü—Ä–∏–∑–Ω–∞–∫', 
        '–û—Ü–µ–Ω–∫–∞ –≤–ª–∏—è–Ω–∏—è',
        '–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ',
        '–¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è'
    ])

def translate_reasoning_to_russian(llm, text):
    """Modified to handle both standard LLMs and FallbackLLMSystem"""
    if isinstance(llm, FallbackLLMSystem):
        # Direct translation using MT5
        response = llm.invoke({
            'template_result': f"Translate to Russian: {text}"
        })
        return response.content.strip()
    else:
        # Original LangChain approach
        template = """
        Translate this English explanation to Russian, maintaining a formal business style:
        "{text}"
        
        Your response should contain only the Russian translation.
        """
        prompt = PromptTemplate(template=template, input_variables=["text"])
        chain = prompt | llm
        response = chain.invoke({"text": text})
        
        # Handle different response types
        if hasattr(response, 'content'):
            return response.content.strip()
        elif isinstance(response, str):
            return response.strip()
        else:
            return str(response).strip()


def create_output_file(df, uploaded_file, llm):
    """Create Excel file with multiple sheets from processed DataFrame"""
    try:
        wb = load_workbook("sample_file.xlsx")
        
        # 1. Update '–ü—É–±–ª–∏–∫–∞—Ü–∏–∏' sheet
        ws = wb['–ü—É–±–ª–∏–∫–∞—Ü–∏–∏']
        for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        # 2. Update '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥' sheet with events
        ws = wb['–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥']
        row_idx = 4
        events_df = df[df['Event_Type'] != '–ù–µ—Ç'].copy()
        for _, row in events_df.iterrows():
            ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])
            ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])
            ws.cell(row=row_idx, column=7, value=row['Event_Type'])
            ws.cell(row=row_idx, column=8, value=row['Event_Summary'])
            ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
            row_idx += 1

        # 3. Update '–°–≤–æ–¥–∫–∞' sheet
        ws = wb['–°–≤–æ–¥–∫–∞']
        entity_stats = pd.DataFrame({
            '–û–±—ä–µ–∫—Ç': df['–û–±—ä–µ–∫—Ç'].unique()
        })
        entity_stats['–í—Å–µ–≥–æ'] = df.groupby('–û–±—ä–µ–∫—Ç').size()
        entity_stats['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ'] = df[df['Sentiment'] == 'Negative'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int)
        entity_stats['–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ'] = df[df['Sentiment'] == 'Positive'].groupby('–û–±—ä–µ–∫—Ç').size().fillna(0).astype(int)
        
        for idx, (entity, row) in enumerate(entity_stats.iterrows(), start=4):
            ws.cell(row=idx, column=5, value=entity)
            ws.cell(row=idx, column=6, value=row['–í—Å–µ–≥–æ'])
            ws.cell(row=idx, column=7, value=row['–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ'])
            ws.cell(row=idx, column=8, value=row['–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ'])
            # Get impact for entity
            entity_df = df[df['–û–±—ä–µ–∫—Ç'] == entity]
            negative_df = entity_df[entity_df['Sentiment'] == 'Negative']
            impact = negative_df['Impact'].iloc[0] if len(negative_df) > 0 else '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç'
            ws.cell(row=idx, column=9, value=impact)

        # 4. Update '–ó–Ω–∞—á–∏–º—ã–µ' sheet
        ws = wb['–ó–Ω–∞—á–∏–º—ã–µ']
        row_idx = 3
        sentiment_df = df[df['Sentiment'].isin(['Negative', 'Positive'])].copy()
        for _, row in sentiment_df.iterrows():
            ws.cell(row=row_idx, column=3, value=row['–û–±—ä–µ–∫—Ç'])
            ws.cell(row=row_idx, column=4, value='—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ')
            ws.cell(row=row_idx, column=5, value=row['Sentiment'])
            ws.cell(row=row_idx, column=6, value=row.get('Impact', '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç'))
            ws.cell(row=row_idx, column=7, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])
            ws.cell(row=row_idx, column=8, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
            row_idx += 1

        # 5. Update '–ê–Ω–∞–ª–∏–∑' sheet
        ws = wb['–ê–Ω–∞–ª–∏–∑']
        row_idx = 4
        negative_df = df[df['Sentiment'] == 'Negative'].copy()
        for _, row in negative_df.iterrows():
            ws.cell(row=row_idx, column=5, value=row['–û–±—ä–µ–∫—Ç'])
            ws.cell(row=row_idx, column=6, value=row['–ó–∞–≥–æ–ª–æ–≤–æ–∫'])
            ws.cell(row=row_idx, column=7, value="–†–∏—Å–∫ —É–±—ã—Ç–∫–∞")
            ws.cell(row=row_idx, column=8, value=row.get('Reasoning', '–ù–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ'))
            ws.cell(row=row_idx, column=9, value=row['–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞'])
            row_idx += 1

        # 6. Update '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' sheet
        if '–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' not in wb.sheetnames:
            wb.create_sheet('–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ')
        ws = wb['–¢–µ—Ö.–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']
        
        tech_cols = ['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', '–í—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞', 'Translated', 'Sentiment', 'Impact', 'Reasoning']
        tech_df = df[tech_cols].copy()
        
        for r_idx, row in enumerate(dataframe_to_rows(tech_df, index=False, header=True), start=1):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        # Save workbook
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)
        return output

    except Exception as e:
        st.error(f"Error creating output file: {str(e)}")
        st.error(f"DataFrame shape: {df.shape}")
        st.error(f"Available columns: {df.columns.tolist()}")
        return None

def main():
    st.set_page_config(layout="wide")
    
    with st.sidebar:
        st.title("::: AI-–∞–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (v.4.10):::")
        st.subheader("–ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –°–ö–ê–ù-–ò–ù–¢–ï–†–§–ê–ö–°")
        
        model_choice = st.radio(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
            ["Local-MT5", "Qwen2.5-Coder", "Groq (llama-3.1-70b)", "ChatGPT-4-mini"],
            key="model_selector",
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"
        )
        
        uploaded_file = st.file_uploader(
            "–í—ã–±–∏—Ä–∞–π—Ç–µ Excel-—Ñ–∞–π–ª",
            type="xlsx",
            key="file_uploader"
        )
        
        st.markdown(
            """
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏:  
            - –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–µ–¥—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π **BERT**
            - –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –ø–æ–º–æ—â–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (**LLM**)
            - –§—Ä–µ–π–º–≤–æ—Ä–∫ **LangChain** –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏
            """,
            unsafe_allow_html=True
        )

        st.markdown(
        """
        <style>
        .signature {
            position: fixed;
            right: 12px;
            down: 12px;
            font-size: 14px;
            color: #FF0000;
            opacity: 0.9;
            z-index: 999;
        }
        </style>
        <div class="signature">denis.pokrovsky.npff</div>
        """,
        unsafe_allow_html=True
        )

    # Main content area
    st.title("–ê–Ω–∞–ª–∏–∑ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    
    # Initialize session state
    if 'processed_df' not in st.session_state:
        st.session_state.processed_df = None
        
    # Create display areas
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Area for real-time updates
        st.subheader("–ß—Ç–æ –Ω–∞–π–¥–µ–Ω–æ, –ø–æ–∫–∞–∑—ã–≤–∞—é:")
        st.markdown("""
            <style>
            .stProgress .st-bo {
                background-color: #f0f2f6;
            }
            .negative-alert {
                background-color: #ffebee;
                border-left: 5px solid #f44336;
                padding: 10px;
                margin: 5px 0;
            }
            .event-alert {
                background-color: #e3f2fd;
                border-left: 5px solid #2196f3;
                padding: 10px;
                margin: 5px 0;
            }
            </style>
        """, unsafe_allow_html=True)
        
    with col2:
        # Area for statistics
        st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
        if st.session_state.processed_df is not None:
            st.metric("–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π", len(st.session_state.processed_df))
            st.metric("–ò–∑ –Ω–∏—Ö –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö", 
                len(st.session_state.processed_df[
                    st.session_state.processed_df['Sentiment'] == 'Negative'
                ])
            )
            st.metric("–°–æ–±—ã—Ç–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", 
                len(st.session_state.processed_df[
                    st.session_state.processed_df['Event_Type'] != '–ù–µ—Ç'
                ])
            )
    
    if uploaded_file is not None and st.session_state.processed_df is None:
        start_time = time.time()
        
        try:
            st.session_state.processed_df = process_file(
                uploaded_file,
                model_choice,
                translation_method='auto'
            )
            
            if st.session_state.processed_df is not None:
                end_time = time.time()
                elapsed_time = format_elapsed_time(end_time - start_time)
                
                # Show results
                st.subheader("–ò—Ç–æ–≥–æ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º")
                
                # Display statistics
                stats_cols = st.columns(4)
                with stats_cols[0]:
                    st.metric("–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ", len(st.session_state.processed_df))
                with stats_cols[1]:
                    st.metric("–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö", 
                        len(st.session_state.processed_df[
                            st.session_state.processed_df['Sentiment'] == 'Negative'
                        ])
                    )
                with stats_cols[2]:
                    st.metric("–°–æ–±—ã—Ç–∏–π –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ", 
                        len(st.session_state.processed_df[
                            st.session_state.processed_df['Event_Type'] != '–ù–µ—Ç'
                        ])
                    )
                with stats_cols[3]:
                    st.metric("–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ—Å—Ç–∞–≤–∏–ª–æ", elapsed_time)
                
                # Show data previews
                with st.expander("üìä –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –¥–∞–Ω–Ω—ã—Ö", expanded=True):
                    preview_cols = ['–û–±—ä–µ–∫—Ç', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'Sentiment', 'Event_Type']
                    st.dataframe(
                        st.session_state.processed_df[preview_cols],
                        use_container_width=True
                    )
                
                # Create downloadable report
                output = create_output_file(
                    st.session_state.processed_df,
                    uploaded_file,
                    init_langchain_llm(model_choice)
                )
                
                st.download_button(
                    label="üì• –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç - –∑–∞–≥—Ä—É–∑–∏—Ç—å",
                    data=output,
                    file_name="—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã_–∞–Ω–∞–ª–∏–∑–∞.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key='download_button'
                )
                
        except Exception as e:
            st.error(f"–û—à–∏–±–æ—á–∫–∞ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
            st.session_state.processed_df = None


if __name__ == "__main__":
    main()